{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wurzel Wurzel is an open-source Python library built to address advanced Extract, Transform, Load (ETL) needs for Retrieval-Augmented Generation (RAG) systems. It is designed to streamline ETL processes while offering essential features like multi-tenancy, cloud-native deployment support, and job scheduling. The repository includes initial implementations for widely-used frameworks in the RAG ecosystem, such as Qdrant, Milvus, and Hugging Face, providing users with a strong starting point for building scalable and efficient RAG pipelines. Features Advanced ETL Pipelines : Tailored for the specific needs of RAG systems. Multi-Tenancy : Easily manage multiple tenants or projects within a single system. Cloud-Native Deployment : Designed for seamless integration with Kubernetes, Docker, and other cloud platforms. Scheduling Capabilities : Schedule and manage ETL tasks using built-in or external tools. Framework Integrations : Pre-built support for popular tools like Qdrant, Milvus, and Hugging Face. Type Security : By leveraging capabilities of pydantic and pandera we ensure type security Naming","title":"Wurzel"},{"location":"#wurzel","text":"Wurzel is an open-source Python library built to address advanced Extract, Transform, Load (ETL) needs for Retrieval-Augmented Generation (RAG) systems. It is designed to streamline ETL processes while offering essential features like multi-tenancy, cloud-native deployment support, and job scheduling. The repository includes initial implementations for widely-used frameworks in the RAG ecosystem, such as Qdrant, Milvus, and Hugging Face, providing users with a strong starting point for building scalable and efficient RAG pipelines.","title":"Wurzel"},{"location":"#features","text":"Advanced ETL Pipelines : Tailored for the specific needs of RAG systems. Multi-Tenancy : Easily manage multiple tenants or projects within a single system. Cloud-Native Deployment : Designed for seamless integration with Kubernetes, Docker, and other cloud platforms. Scheduling Capabilities : Schedule and manage ETL tasks using built-in or external tools. Framework Integrations : Pre-built support for popular tools like Qdrant, Milvus, and Hugging Face. Type Security : By leveraging capabilities of pydantic and pandera we ensure type security","title":"Features"},{"location":"#naming","text":"","title":"Naming"},{"location":"developping/Building_your_one_step/","text":"\ud83d\udcda How to Build Your Own Step Wurzel provides a modular pipeline system where each unit of processing is encapsulated in a Step. There are two main types of steps: Datasource Steps (WurzelTips): These are entry points to your pipeline, ingesting data from external sources. Processing Steps (WurzelSteps): These consume and transform data from earlier steps. \u2699\ufe0f Initialization Logic The run method of each step may be executed multiple times\u2014once per upstream dependency. If your step needs to perform setup actions (e.g., creating database tables or opening persistent connections), implement that logic in the constructor ( init ) instead of run. \ud83d\uddc3\ufe0f Example: Step with Initialization class MyDatabaseStep(TypedStep[DatabaseSettings, DataFrame[EmbeddingResult], DataFrame[EmbeddingResult]]): def __init__(self): # Initialize database connections, create tables, etc. self.connection = establish_connection() self.ensure_tables() def run(self, inpt: DataFrame[EmbeddingResult]) -> DataFrame[EmbeddingResult]: # Insert data into database or perform other processing return inpt \ud83d\udee0\ufe0f Finalization Logic Each step also provides a finalize method, which is called after the execution in the Executor has finished. This method can be used for cleanup or other post-processing tasks. \ud83d\uddc3\ufe0f Example: Step with Finalization class MyDatabaseStep(TypedStep[DatabaseSettings, DataFrame[EmbeddingResult], DataFrame[EmbeddingResult]]): def __init__(self): # Initialize database connections, create tables, etc. self.connection = establish_connection() self.ensure_tables() def run(self, inpt: DataFrame[EmbeddingResult]) -> DataFrame[EmbeddingResult]: # Insert data into database or perform other processing return inpt def finalize(self) -> None: # Cleanup logic, such as retiring collections or closing connections self.connection.close() \ud83e\uddf1 Creating a New WurzelTip (Datasource Step) A WurzelTip is a step that introduces data into the pipeline. Since it does not rely on any prior step, its InputDataContract is always None. By convention, we use MarkdownDataContract as the initial data format for document retrieval pipelines, but you are free to define and use your own custom contracts. \u2705 Requirements Settings: Optional configuration schema using Pydantic's BaseModel (via Settings). InputDataContract: Always None for data sources. OutputDataContract: Required. Typically list[MarkdownDataContract]. \ud83d\udce6 Example class MySettings(Settings): \"\"\"Configuration for MyDatasourceStep\"\"\" YOUR_REQUIRED_ENVIRONMENT: Any class MyDatasourceStep(TypedStep[MySettings, None, list[MarkdownDataContract]]): \"\"\"Data source step for loading Markdown files from a configurable path.\"\"\" def run(self, inpt: None) -> list[MarkdownDataContract]: # Your custom data ingestion logic here return result \ud83d\udd01 Creating a New WurzelStep (Processing Step) A WurzelStep consumes the output of a previous step, performs a transformation, and passes its output to the next step in the pipeline. These can include: Filters: Narrow down data based on a condition. Validators: Check for schema or content correctness. Transformers: Change the data structure, such as converting from a list of documents to a DataFrame of embeddings. \ud83e\uddf9 Example: Filter Step class MyFilterStep(TypedStep[MySettings, list[MarkdownDataContract], list[MarkdownDataContract]]): def run(self, inpt: list[MarkdownDataContract]) -> list[MarkdownDataContract]: # Your filtering logic here return result \ud83d\udd04 Example: Transformation Step Sometimes a step will change the shape or structure of the data entirely, for example transforming a list of documents into a pandas.DataFrame. This is common in steps that perform embedding or analytics. class MyEmbeddingStep(TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]]): def run(self, inpt: list[MarkdownDataContract]) -> DataFrame[EmbeddingResult]: \"\"\"Transforms input markdown files into embeddings stored in a DataFrame.\"\"\" # Your embedding logic here return df","title":"\ud83d\udcda How to Build Your Own Step"},{"location":"developping/Building_your_one_step/#how-to-build-your-own-step","text":"Wurzel provides a modular pipeline system where each unit of processing is encapsulated in a Step. There are two main types of steps: Datasource Steps (WurzelTips): These are entry points to your pipeline, ingesting data from external sources. Processing Steps (WurzelSteps): These consume and transform data from earlier steps.","title":"\ud83d\udcda How to Build Your Own Step"},{"location":"developping/Building_your_one_step/#initialization-logic","text":"The run method of each step may be executed multiple times\u2014once per upstream dependency. If your step needs to perform setup actions (e.g., creating database tables or opening persistent connections), implement that logic in the constructor ( init ) instead of run.","title":"\u2699\ufe0f Initialization Logic"},{"location":"developping/Building_your_one_step/#example-step-with-initialization","text":"class MyDatabaseStep(TypedStep[DatabaseSettings, DataFrame[EmbeddingResult], DataFrame[EmbeddingResult]]): def __init__(self): # Initialize database connections, create tables, etc. self.connection = establish_connection() self.ensure_tables() def run(self, inpt: DataFrame[EmbeddingResult]) -> DataFrame[EmbeddingResult]: # Insert data into database or perform other processing return inpt","title":"\ud83d\uddc3\ufe0f Example: Step with Initialization"},{"location":"developping/Building_your_one_step/#finalization-logic","text":"Each step also provides a finalize method, which is called after the execution in the Executor has finished. This method can be used for cleanup or other post-processing tasks.","title":"\ud83d\udee0\ufe0f Finalization Logic"},{"location":"developping/Building_your_one_step/#example-step-with-finalization","text":"class MyDatabaseStep(TypedStep[DatabaseSettings, DataFrame[EmbeddingResult], DataFrame[EmbeddingResult]]): def __init__(self): # Initialize database connections, create tables, etc. self.connection = establish_connection() self.ensure_tables() def run(self, inpt: DataFrame[EmbeddingResult]) -> DataFrame[EmbeddingResult]: # Insert data into database or perform other processing return inpt def finalize(self) -> None: # Cleanup logic, such as retiring collections or closing connections self.connection.close()","title":"\ud83d\uddc3\ufe0f Example: Step with Finalization"},{"location":"developping/Building_your_one_step/#creating-a-new-wurzeltip-datasource-step","text":"A WurzelTip is a step that introduces data into the pipeline. Since it does not rely on any prior step, its InputDataContract is always None. By convention, we use MarkdownDataContract as the initial data format for document retrieval pipelines, but you are free to define and use your own custom contracts.","title":"\ud83e\uddf1 Creating a New WurzelTip (Datasource Step)"},{"location":"developping/Building_your_one_step/#requirements","text":"Settings: Optional configuration schema using Pydantic's BaseModel (via Settings). InputDataContract: Always None for data sources. OutputDataContract: Required. Typically list[MarkdownDataContract].","title":"\u2705 Requirements"},{"location":"developping/Building_your_one_step/#example","text":"class MySettings(Settings): \"\"\"Configuration for MyDatasourceStep\"\"\" YOUR_REQUIRED_ENVIRONMENT: Any class MyDatasourceStep(TypedStep[MySettings, None, list[MarkdownDataContract]]): \"\"\"Data source step for loading Markdown files from a configurable path.\"\"\" def run(self, inpt: None) -> list[MarkdownDataContract]: # Your custom data ingestion logic here return result","title":"\ud83d\udce6 Example"},{"location":"developping/Building_your_one_step/#creating-a-new-wurzelstep-processing-step","text":"A WurzelStep consumes the output of a previous step, performs a transformation, and passes its output to the next step in the pipeline. These can include: Filters: Narrow down data based on a condition. Validators: Check for schema or content correctness. Transformers: Change the data structure, such as converting from a list of documents to a DataFrame of embeddings.","title":"\ud83d\udd01 Creating a New WurzelStep (Processing Step)"},{"location":"developping/Building_your_one_step/#example-filter-step","text":"class MyFilterStep(TypedStep[MySettings, list[MarkdownDataContract], list[MarkdownDataContract]]): def run(self, inpt: list[MarkdownDataContract]) -> list[MarkdownDataContract]: # Your filtering logic here return result","title":"\ud83e\uddf9 Example: Filter Step"},{"location":"developping/Building_your_one_step/#example-transformation-step","text":"Sometimes a step will change the shape or structure of the data entirely, for example transforming a list of documents into a pandas.DataFrame. This is common in steps that perform embedding or analytics. class MyEmbeddingStep(TypedStep[EmbeddingSettings, list[MarkdownDataContract], DataFrame[EmbeddingResult]]): def run(self, inpt: list[MarkdownDataContract]) -> DataFrame[EmbeddingResult]: \"\"\"Transforms input markdown files into embeddings stored in a DataFrame.\"\"\" # Your embedding logic here return df","title":"\ud83d\udd04 Example: Transformation Step"},{"location":"developping/define_pipline/","text":"\ud83d\udd17 Defining a Pipeline in Wurzel At the heart of Wurzel lies the concept of the pipeline \u2014 a chain of steps that are connected and executed in sequence. Each step processes the output of the previous one, enabling modular, reusable, and optimally scheduled workflows. \ud83e\udde9 What is a Wurzel Pipeline? A pipeline in Wurzel is a chain of TypedStep instances, linked using the >> operator. This chaining mechanism makes it easy to define complex data processing flows that remain clean and composable. Wurzel optimizes the execution of these pipelines automatically based on dependencies and contracts. \ud83d\udee0\ufe0f How to Define a Pipeline To define a pipeline: Instantiate your steps using the helper WZ(...). Chain them together using >>. Return the final step (which implicitly carries the full chain). \ud83d\udce6 Example from wurzel.steps import ( EmbeddingStep, QdrantConnectorStep, ) from wurzel.utils import WZ from wurzel.steps.manual_markdown import ManualMarkdownStep from wurzel.step import TypedStep def pipeline() -> TypedStep: \"\"\"Defines a Wurzel pipeline that embeds manual markdown and stores it in Qdrant.\"\"\" # Step 1: Load markdown input manually md = WZ(ManualMarkdownStep) # Step 2: Generate embeddings from markdown content embed = WZ(EmbeddingStep) # Step 3: Store embeddings in a Qdrant vector database db = WZ(QdrantConnectorStep) # Chain the steps md >> embed >> db # Return the final step in the chain return db \ud83d\udd04 Execution Order Even though the function returns only the last step (db), Wurzel automatically resolves and runs all upstream dependencies in the correct order: ManualMarkdownStep runs first to provide data. EmbeddingStep transforms that data into vectors. QdrantConnectorStep persists the result.","title":"\ud83d\udd17 Defining a Pipeline in Wurzel"},{"location":"developping/define_pipline/#defining-a-pipeline-in-wurzel","text":"At the heart of Wurzel lies the concept of the pipeline \u2014 a chain of steps that are connected and executed in sequence. Each step processes the output of the previous one, enabling modular, reusable, and optimally scheduled workflows.","title":"\ud83d\udd17 Defining a Pipeline in Wurzel"},{"location":"developping/define_pipline/#what-is-a-wurzel-pipeline","text":"A pipeline in Wurzel is a chain of TypedStep instances, linked using the >> operator. This chaining mechanism makes it easy to define complex data processing flows that remain clean and composable. Wurzel optimizes the execution of these pipelines automatically based on dependencies and contracts.","title":"\ud83e\udde9 What is a Wurzel Pipeline?"},{"location":"developping/define_pipline/#how-to-define-a-pipeline","text":"To define a pipeline: Instantiate your steps using the helper WZ(...). Chain them together using >>. Return the final step (which implicitly carries the full chain).","title":"\ud83d\udee0\ufe0f How to Define a Pipeline"},{"location":"developping/define_pipline/#example","text":"from wurzel.steps import ( EmbeddingStep, QdrantConnectorStep, ) from wurzel.utils import WZ from wurzel.steps.manual_markdown import ManualMarkdownStep from wurzel.step import TypedStep def pipeline() -> TypedStep: \"\"\"Defines a Wurzel pipeline that embeds manual markdown and stores it in Qdrant.\"\"\" # Step 1: Load markdown input manually md = WZ(ManualMarkdownStep) # Step 2: Generate embeddings from markdown content embed = WZ(EmbeddingStep) # Step 3: Store embeddings in a Qdrant vector database db = WZ(QdrantConnectorStep) # Chain the steps md >> embed >> db # Return the final step in the chain return db","title":"\ud83d\udce6 Example"},{"location":"developping/define_pipline/#execution-order","text":"Even though the function returns only the last step (db), Wurzel automatically resolves and runs all upstream dependencies in the correct order: ManualMarkdownStep runs first to provide data. EmbeddingStep transforms that data into vectors. QdrantConnectorStep persists the result.","title":"\ud83d\udd04 Execution Order"},{"location":"developping/docs/","text":"\ud83d\udcd6 Documentation Wurzel uses MkDocs to manage and build its project documentation. MkDocs is a fast, simple, and elegant static site generator that's perfect for documentation projects. \ud83e\uddea Serving Documentation Locally To preview the documentation on your local machine (for development or testing), use the following command: make documentation","title":"\ud83d\udcd6 Documentation"},{"location":"developping/docs/#documentation","text":"Wurzel uses MkDocs to manage and build its project documentation. MkDocs is a fast, simple, and elegant static site generator that's perfect for documentation projects.","title":"\ud83d\udcd6 Documentation"},{"location":"developping/docs/#serving-documentation-locally","text":"To preview the documentation on your local machine (for development or testing), use the following command: make documentation","title":"\ud83e\uddea Serving Documentation Locally"},{"location":"developping/getting_started/","text":"\ud83d\udc69\u200d\ud83d\udcbb Developer Guide Welcome to the Wurzel Developer Guide! This document provides the essential steps and best practices to help you contribute effectively to the Wurzel project. \ud83d\ude80 Getting Started To install Wurzel with all necessary dependencies, run: make install This will install the core library along with all optional extras used for development, testing, and documentation. \ud83e\uddfc Code Quality: Linting Wurzel uses pre-commit hooks to enforce consistent code quality and formatting. \u2705 Set Up Pre-commit Hooks To activate the hooks: pre-commit install \ud83e\uddea Run Linting Manually You can also trigger the linting process manually using: make lint This runs all configured linters and formatters across the codebase. \ud83e\uddea Running Tests Before submitting your changes, make sure all tests pass: make test This runs the full test suite and ensures your changes don\u2019t break existing functionality. \ud83d\udcdd Commit Strategy We maintain a clean and readable Git history by squashing all commits when merging into the main branch. \ud83d\udd16 Commit Types Breaking Changes : For changes that are not backward-compatible. Use the tag: breaking Features : For new features or enhancements that are backward-compatible. Use the tags: feat , feature Fixes and Improvements : For bug fixes, performance improvements, or small patches. Use the tags: fix , perf , hotfix , patch Allowed Tags To ensure consistency, the following tags are allowed in commit messages: - feat , feature , fix , hotfix , perf , patch , build , chore , ci , docs , style , refactor , ref , test Commit Message Format Commit messages should follow this structure: <tag>: <short description> <longer description (optional)> Adhering to this format ensures clarity and improves the readability of the project's history.","title":"\ud83d\udc69\u200d\ud83d\udcbb Developer Guide"},{"location":"developping/getting_started/#developer-guide","text":"Welcome to the Wurzel Developer Guide! This document provides the essential steps and best practices to help you contribute effectively to the Wurzel project.","title":"\ud83d\udc69\u200d\ud83d\udcbb Developer Guide"},{"location":"developping/getting_started/#getting-started","text":"To install Wurzel with all necessary dependencies, run: make install This will install the core library along with all optional extras used for development, testing, and documentation.","title":"\ud83d\ude80 Getting Started"},{"location":"developping/getting_started/#code-quality-linting","text":"Wurzel uses pre-commit hooks to enforce consistent code quality and formatting.","title":"\ud83e\uddfc Code Quality: Linting"},{"location":"developping/getting_started/#set-up-pre-commit-hooks","text":"To activate the hooks: pre-commit install","title":"\u2705 Set Up Pre-commit Hooks"},{"location":"developping/getting_started/#run-linting-manually","text":"You can also trigger the linting process manually using: make lint This runs all configured linters and formatters across the codebase.","title":"\ud83e\uddea Run Linting Manually"},{"location":"developping/getting_started/#running-tests","text":"Before submitting your changes, make sure all tests pass: make test This runs the full test suite and ensures your changes don\u2019t break existing functionality.","title":"\ud83e\uddea Running Tests"},{"location":"developping/getting_started/#commit-strategy","text":"We maintain a clean and readable Git history by squashing all commits when merging into the main branch.","title":"\ud83d\udcdd Commit Strategy"},{"location":"developping/getting_started/#commit-types","text":"Breaking Changes : For changes that are not backward-compatible. Use the tag: breaking Features : For new features or enhancements that are backward-compatible. Use the tags: feat , feature Fixes and Improvements : For bug fixes, performance improvements, or small patches. Use the tags: fix , perf , hotfix , patch","title":"\ud83d\udd16 Commit Types"},{"location":"developping/getting_started/#allowed-tags","text":"To ensure consistency, the following tags are allowed in commit messages: - feat , feature , fix , hotfix , perf , patch , build , chore , ci , docs , style , refactor , ref , test","title":"Allowed Tags"},{"location":"developping/getting_started/#commit-message-format","text":"Commit messages should follow this structure: <tag>: <short description> <longer description (optional)> Adhering to this format ensures clarity and improves the readability of the project's history.","title":"Commit Message Format"},{"location":"steps/docling/","text":"Note: Known Limitations with EasyOCR ( EasyOcrOptions ). Table structure is often lost or misaligned in the OCR output. Spelling inaccuracies are occasionally observed (e.g., \"Verl\u00e4ngern\" \u2192 \"Verl\u00e4ngenng\"). URLs are not parsed correctly (e.g., \"www.telekom.de/agb\" \u2192 \"www telekom delagb\"). While investigating EasyOCR issues and testing alternative OCR engines, we observed that some documents produced distorted text with irregular whitespace. This disrupts the natural sentence flow and significantly reduces readability. Example: \"pra kti sche i nform ati o nen zu i h rer fam i l y card basi c Li eber Tel ekom Kunde, sch\u00f6n, dass Si e si ch f \u00fcr...\" Despite these limitations, we have decided to proceed with EasyOCR. CleanMarkdownRenderer Bases: HTMLRenderer Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input. Source code in wurzel/steps/docling/docling_step.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class CleanMarkdownRenderer ( HTMLRenderer ): \"\"\"Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input. \"\"\" @staticmethod def render_html_block ( token ): \"\"\"Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like <!-- image --> \"\"\" soup = BeautifulSoup ( token . content , \"html.parser\" ) for comment in soup . find_all ( string = lambda text : isinstance ( text , Comment )): comment . extract () return soup . decode_contents () . strip () render_html_block ( token ) staticmethod Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like Source code in wurzel/steps/docling/docling_step.py 49 50 51 52 53 54 55 56 57 58 59 60 @staticmethod def render_html_block ( token ): \"\"\"Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like <!-- image --> \"\"\" soup = BeautifulSoup ( token . content , \"html.parser\" ) for comment in soup . find_all ( string = lambda text : isinstance ( text , Comment )): comment . extract () return soup . decode_contents () . strip () DoclingStep Bases: TypedStep [ DoclingSettings , None, list [ MarkdownDataContract ]] Step to return local Markdown files with enhanced PDF extraction for German. Source code in wurzel/steps/docling/docling_step.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class DoclingStep ( TypedStep [ DoclingSettings , None , list [ MarkdownDataContract ]]): \"\"\"Step to return local Markdown files with enhanced PDF extraction for German.\"\"\" def __init__ ( self ): super () . __init__ () self . converter = self . create_converter () def create_converter ( self ) -> DocumentConverter : \"\"\"Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter: Configured document converter. \"\"\" pipeline_options = PdfPipelineOptions () ocr_options = EasyOcrOptions () pipeline_options . ocr_options = ocr_options return DocumentConverter ( allowed_formats = self . settings . FORMATS , format_options = { InputFormat . PDF : PdfFormatOption ( pipeline_options = pipeline_options , ) }, ) @staticmethod def extract_keywords ( md_text : str ) -> str : \"\"\"Cleans a Markdown string using mistletoe and extracts useful content. - Parses and renders the Markdown content into HTML using a custom HTML renderer - Removes unwanted HTML comments and escaped underscores - Extracts the first heading from the content (e.g., `<h1>` to `<h6>`) - Converts the cleaned HTML into plain text Args: md_text (str): The raw Markdown input string. \"\"\" with MD_RENDER_LOCK , CleanMarkdownRenderer () as renderer : ast = MTDocument ( md_text ) cleaned = renderer . render ( ast ) . replace ( \" \\n \" , \"\" ) soup = BeautifulSoup ( cleaned , \"html.parser\" ) first_heading_tag = soup . find ([ \"h1\" , \"h2\" , \"h3\" , \"h4\" , \"h5\" , \"h6\" ]) heading = first_heading_tag . get_text ( strip = True ) if first_heading_tag else \"\" return heading def run ( self , inpt : None ) -> list [ MarkdownDataContract ]: \"\"\"Run the document extraction and conversion process for German PDFs. Args: inpt (None): Input parameter (not used). Returns: List[MarkdownDataContract]: List of converted Markdown contracts. \"\"\" urls = self . settings . URLS contracts = [] for url in urls : try : converted_contract = self . converter . convert ( url ) md = converted_contract . document . export_to_markdown ( image_placeholder = \"\" ) keyword = self . extract_keywords ( md ) contract_instance = { \"md\" : md , \"keywords\" : \" \" . join ([ self . settings . DEFAULT_KEYWORD , keyword ]), \"url\" : url } contracts . append ( contract_instance ) except ( FileNotFoundError , OSError ) as e : log . error ( f \"Failed to verify URL: { url } . Error: { e } \" ) continue return contracts create_converter () Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter ( DocumentConverter ) \u2013 Configured document converter. Source code in wurzel/steps/docling/docling_step.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def create_converter ( self ) -> DocumentConverter : \"\"\"Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter: Configured document converter. \"\"\" pipeline_options = PdfPipelineOptions () ocr_options = EasyOcrOptions () pipeline_options . ocr_options = ocr_options return DocumentConverter ( allowed_formats = self . settings . FORMATS , format_options = { InputFormat . PDF : PdfFormatOption ( pipeline_options = pipeline_options , ) }, ) extract_keywords ( md_text ) staticmethod Cleans a Markdown string using mistletoe and extracts useful content. Parses and renders the Markdown content into HTML using a custom HTML renderer Removes unwanted HTML comments and escaped underscores Extracts the first heading from the content (e.g., <h1> to <h6> ) Converts the cleaned HTML into plain text Parameters: md_text ( str ) \u2013 The raw Markdown input string. Source code in wurzel/steps/docling/docling_step.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @staticmethod def extract_keywords ( md_text : str ) -> str : \"\"\"Cleans a Markdown string using mistletoe and extracts useful content. - Parses and renders the Markdown content into HTML using a custom HTML renderer - Removes unwanted HTML comments and escaped underscores - Extracts the first heading from the content (e.g., `<h1>` to `<h6>`) - Converts the cleaned HTML into plain text Args: md_text (str): The raw Markdown input string. \"\"\" with MD_RENDER_LOCK , CleanMarkdownRenderer () as renderer : ast = MTDocument ( md_text ) cleaned = renderer . render ( ast ) . replace ( \" \\n \" , \"\" ) soup = BeautifulSoup ( cleaned , \"html.parser\" ) first_heading_tag = soup . find ([ \"h1\" , \"h2\" , \"h3\" , \"h4\" , \"h5\" , \"h6\" ]) heading = first_heading_tag . get_text ( strip = True ) if first_heading_tag else \"\" return heading run ( inpt ) Run the document extraction and conversion process for German PDFs. Parameters: inpt ( None ) \u2013 Input parameter (not used). Returns: list [ MarkdownDataContract ] \u2013 List[MarkdownDataContract]: List of converted Markdown contracts. Source code in wurzel/steps/docling/docling_step.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def run ( self , inpt : None ) -> list [ MarkdownDataContract ]: \"\"\"Run the document extraction and conversion process for German PDFs. Args: inpt (None): Input parameter (not used). Returns: List[MarkdownDataContract]: List of converted Markdown contracts. \"\"\" urls = self . settings . URLS contracts = [] for url in urls : try : converted_contract = self . converter . convert ( url ) md = converted_contract . document . export_to_markdown ( image_placeholder = \"\" ) keyword = self . extract_keywords ( md ) contract_instance = { \"md\" : md , \"keywords\" : \" \" . join ([ self . settings . DEFAULT_KEYWORD , keyword ]), \"url\" : url } contracts . append ( contract_instance ) except ( FileNotFoundError , OSError ) as e : log . error ( f \"Failed to verify URL: { url } . Error: { e } \" ) continue return contracts Specific docling settings. DoclingSettings Bases: Settings DoclingSettings is a configuration class that inherits from the base Settings class. It provides customizable settings for document processing. Attributes: FORCE_FULL_PAGE_OCR ( bool ) \u2013 A flag to enforce full-page OCR processing. Defaults to True. FORMATS ( list [ InputFormat ] ) \u2013 A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\" URLS ( list [ str ] ) \u2013 A list of URLs for additional configuration or resources. Defaults to an empty list. Source code in wurzel/steps/docling/settings.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class DoclingSettings ( Settings ): \"\"\"DoclingSettings is a configuration class that inherits from the base `Settings` class. It provides customizable settings for document processing. Attributes: FORCE_FULL_PAGE_OCR (bool): A flag to enforce full-page OCR processing. Defaults to True. FORMATS (list[InputFormat]): A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\" URLS (list[str]): A list of URLs for additional configuration or resources. Defaults to an empty list. \"\"\" FORCE_FULL_PAGE_OCR : bool = True FORMATS : list [ InputFormat ] = [ \"docx\" , \"asciidoc\" , \"pptx\" , \"html\" , \"image\" , \"pdf\" , \"md\" , \"csv\" , \"xlsx\" , \"xml_uspto\" , \"xml_jats\" , \"json_docling\" , ] URLS : list [ str ] = [] DEFAULT_KEYWORD : str = \"\"","title":"Docling"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer","text":"Bases: HTMLRenderer Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input. Source code in wurzel/steps/docling/docling_step.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class CleanMarkdownRenderer ( HTMLRenderer ): \"\"\"Custom Markdown renderer extending mistletoe's HTMLRenderer to clean up unwanted elements from Markdown input. \"\"\" @staticmethod def render_html_block ( token ): \"\"\"Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like <!-- image --> \"\"\" soup = BeautifulSoup ( token . content , \"html.parser\" ) for comment in soup . find_all ( string = lambda text : isinstance ( text , Comment )): comment . extract () return soup . decode_contents () . strip ()","title":"CleanMarkdownRenderer"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.CleanMarkdownRenderer.render_html_block","text":"Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like Source code in wurzel/steps/docling/docling_step.py 49 50 51 52 53 54 55 56 57 58 59 60 @staticmethod def render_html_block ( token ): \"\"\"Render HTML block tokens and clean up unwanted elements. This method removes HTML comments and returns the cleaned HTML content. Remove comments like <!-- image --> \"\"\" soup = BeautifulSoup ( token . content , \"html.parser\" ) for comment in soup . find_all ( string = lambda text : isinstance ( text , Comment )): comment . extract () return soup . decode_contents () . strip ()","title":"render_html_block"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep","text":"Bases: TypedStep [ DoclingSettings , None, list [ MarkdownDataContract ]] Step to return local Markdown files with enhanced PDF extraction for German. Source code in wurzel/steps/docling/docling_step.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class DoclingStep ( TypedStep [ DoclingSettings , None , list [ MarkdownDataContract ]]): \"\"\"Step to return local Markdown files with enhanced PDF extraction for German.\"\"\" def __init__ ( self ): super () . __init__ () self . converter = self . create_converter () def create_converter ( self ) -> DocumentConverter : \"\"\"Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter: Configured document converter. \"\"\" pipeline_options = PdfPipelineOptions () ocr_options = EasyOcrOptions () pipeline_options . ocr_options = ocr_options return DocumentConverter ( allowed_formats = self . settings . FORMATS , format_options = { InputFormat . PDF : PdfFormatOption ( pipeline_options = pipeline_options , ) }, ) @staticmethod def extract_keywords ( md_text : str ) -> str : \"\"\"Cleans a Markdown string using mistletoe and extracts useful content. - Parses and renders the Markdown content into HTML using a custom HTML renderer - Removes unwanted HTML comments and escaped underscores - Extracts the first heading from the content (e.g., `<h1>` to `<h6>`) - Converts the cleaned HTML into plain text Args: md_text (str): The raw Markdown input string. \"\"\" with MD_RENDER_LOCK , CleanMarkdownRenderer () as renderer : ast = MTDocument ( md_text ) cleaned = renderer . render ( ast ) . replace ( \" \\n \" , \"\" ) soup = BeautifulSoup ( cleaned , \"html.parser\" ) first_heading_tag = soup . find ([ \"h1\" , \"h2\" , \"h3\" , \"h4\" , \"h5\" , \"h6\" ]) heading = first_heading_tag . get_text ( strip = True ) if first_heading_tag else \"\" return heading def run ( self , inpt : None ) -> list [ MarkdownDataContract ]: \"\"\"Run the document extraction and conversion process for German PDFs. Args: inpt (None): Input parameter (not used). Returns: List[MarkdownDataContract]: List of converted Markdown contracts. \"\"\" urls = self . settings . URLS contracts = [] for url in urls : try : converted_contract = self . converter . convert ( url ) md = converted_contract . document . export_to_markdown ( image_placeholder = \"\" ) keyword = self . extract_keywords ( md ) contract_instance = { \"md\" : md , \"keywords\" : \" \" . join ([ self . settings . DEFAULT_KEYWORD , keyword ]), \"url\" : url } contracts . append ( contract_instance ) except ( FileNotFoundError , OSError ) as e : log . error ( f \"Failed to verify URL: { url } . Error: { e } \" ) continue return contracts","title":"DoclingStep"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.create_converter","text":"Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter ( DocumentConverter ) \u2013 Configured document converter. Source code in wurzel/steps/docling/docling_step.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def create_converter ( self ) -> DocumentConverter : \"\"\"Create and configure the document converter for PDF and DOCX. Returns: DocumentConverter: Configured document converter. \"\"\" pipeline_options = PdfPipelineOptions () ocr_options = EasyOcrOptions () pipeline_options . ocr_options = ocr_options return DocumentConverter ( allowed_formats = self . settings . FORMATS , format_options = { InputFormat . PDF : PdfFormatOption ( pipeline_options = pipeline_options , ) }, )","title":"create_converter"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.extract_keywords","text":"Cleans a Markdown string using mistletoe and extracts useful content. Parses and renders the Markdown content into HTML using a custom HTML renderer Removes unwanted HTML comments and escaped underscores Extracts the first heading from the content (e.g., <h1> to <h6> ) Converts the cleaned HTML into plain text Parameters: md_text ( str ) \u2013 The raw Markdown input string. Source code in wurzel/steps/docling/docling_step.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @staticmethod def extract_keywords ( md_text : str ) -> str : \"\"\"Cleans a Markdown string using mistletoe and extracts useful content. - Parses and renders the Markdown content into HTML using a custom HTML renderer - Removes unwanted HTML comments and escaped underscores - Extracts the first heading from the content (e.g., `<h1>` to `<h6>`) - Converts the cleaned HTML into plain text Args: md_text (str): The raw Markdown input string. \"\"\" with MD_RENDER_LOCK , CleanMarkdownRenderer () as renderer : ast = MTDocument ( md_text ) cleaned = renderer . render ( ast ) . replace ( \" \\n \" , \"\" ) soup = BeautifulSoup ( cleaned , \"html.parser\" ) first_heading_tag = soup . find ([ \"h1\" , \"h2\" , \"h3\" , \"h4\" , \"h5\" , \"h6\" ]) heading = first_heading_tag . get_text ( strip = True ) if first_heading_tag else \"\" return heading","title":"extract_keywords"},{"location":"steps/docling/#wurzel.steps.docling.docling_step.DoclingStep.run","text":"Run the document extraction and conversion process for German PDFs. Parameters: inpt ( None ) \u2013 Input parameter (not used). Returns: list [ MarkdownDataContract ] \u2013 List[MarkdownDataContract]: List of converted Markdown contracts. Source code in wurzel/steps/docling/docling_step.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def run ( self , inpt : None ) -> list [ MarkdownDataContract ]: \"\"\"Run the document extraction and conversion process for German PDFs. Args: inpt (None): Input parameter (not used). Returns: List[MarkdownDataContract]: List of converted Markdown contracts. \"\"\" urls = self . settings . URLS contracts = [] for url in urls : try : converted_contract = self . converter . convert ( url ) md = converted_contract . document . export_to_markdown ( image_placeholder = \"\" ) keyword = self . extract_keywords ( md ) contract_instance = { \"md\" : md , \"keywords\" : \" \" . join ([ self . settings . DEFAULT_KEYWORD , keyword ]), \"url\" : url } contracts . append ( contract_instance ) except ( FileNotFoundError , OSError ) as e : log . error ( f \"Failed to verify URL: { url } . Error: { e } \" ) continue return contracts Specific docling settings.","title":"run"},{"location":"steps/docling/#wurzel.steps.docling.settings.DoclingSettings","text":"Bases: Settings DoclingSettings is a configuration class that inherits from the base Settings class. It provides customizable settings for document processing. Attributes: FORCE_FULL_PAGE_OCR ( bool ) \u2013 A flag to enforce full-page OCR processing. Defaults to True. FORMATS ( list [ InputFormat ] ) \u2013 A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\" URLS ( list [ str ] ) \u2013 A list of URLs for additional configuration or resources. Defaults to an empty list. Source code in wurzel/steps/docling/settings.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class DoclingSettings ( Settings ): \"\"\"DoclingSettings is a configuration class that inherits from the base `Settings` class. It provides customizable settings for document processing. Attributes: FORCE_FULL_PAGE_OCR (bool): A flag to enforce full-page OCR processing. Defaults to True. FORMATS (list[InputFormat]): A list of supported input formats for document processing. Supported formats include: - \"docx\" - \"asciidoc\" - \"pptx\" - \"html\" - \"image\" - \"pdf\" - \"md\" - \"csv\" - \"xlsx\" - \"xml_uspto\" - \"xml_jats\" - \"json_docling\" URLS (list[str]): A list of URLs for additional configuration or resources. Defaults to an empty list. \"\"\" FORCE_FULL_PAGE_OCR : bool = True FORMATS : list [ InputFormat ] = [ \"docx\" , \"asciidoc\" , \"pptx\" , \"html\" , \"image\" , \"pdf\" , \"md\" , \"csv\" , \"xlsx\" , \"xml_uspto\" , \"xml_jats\" , \"json_docling\" , ] URLS : list [ str ] = [] DEFAULT_KEYWORD : str = \"\"","title":"DoclingSettings"},{"location":"steps/embedding/","text":"consists of DVCSteps to embedd files and save them as for example as csv. Embedded Bases: TypedDict dict definition of a embedded document. Source code in wurzel/steps/embedding/step.py 33 34 35 36 37 38 class Embedded ( TypedDict ): \"\"\"dict definition of a embedded document.\"\"\" text : str url : str vector : list [ float ] EmbeddingStep Bases: SimpleSplitterStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingResult ]] Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult]. Source code in wurzel/steps/embedding/step.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class EmbeddingStep ( SimpleSplitterStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingResult ]], ): \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult]. \"\"\" embedding : HuggingFaceInferenceAPIEmbeddings n_jobs : int markdown : Markdown stopwords : list [ str ] settings : EmbeddingSettings def __init__ ( self ) -> None : super () . __init__ () self . settings = EmbeddingSettings () self . embedding = self . _select_embedding () self . n_jobs = max ( 1 , ( os . cpu_count () or 0 ) - 1 ) # Inject net output_format into 3rd party library Markdown Markdown . output_formats [ \"plain\" ] = self . __md_to_plain # type: ignore[index] self . markdown = Markdown ( output_format = \"plain\" ) # type: ignore[arg-type] self . markdown . stripTopLevelTags = False self . settingstopwords = self . _load_stopwords () self . splitter = SemanticSplitter ( token_limit = self . settings . TOKEN_COUNT_MAX , token_limit_buffer = self . settings . TOKEN_COUNT_BUFFER , token_limit_min = self . settings . TOKEN_COUNT_MIN , ) def _load_stopwords ( self ) -> list [ str ]: path = self . settings . STEPWORDS_PATH with path . open ( encoding = \"utf-8\" ) as f : stopwords = [ w . strip () for w in f . readlines () if not w . startswith ( \";\" )] return stopwords def _select_embedding ( self ) -> HuggingFaceInferenceAPIEmbeddings : \"\"\"Selects the embedding model to be used for generating embeddings. Returns ------- Embeddings An instance of the Embeddings class. \"\"\" return PrefixedAPIEmbeddings ( self . settings . API , self . settings . PREFIX_MAP ) def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingResult ]: \"\"\"Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. \"\"\" if len ( inpt ) == 0 : log . info ( \"Got empty result in Embedding - Skipping\" ) return DataFrame [ EmbeddingResult ]([]) splitted_md_rows = self . _split_markdown ( inpt ) rows = [] failed = 0 for row in tqdm ( splitted_md_rows , desc = \"Calculate Embeddings\" ): try : rows . append ( self . _get_embedding ( row )) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( row )}, ) failed += 1 if failed : log . warning ( f \" { failed } / { len ( splitted_md_rows ) } got skipped\" ) if failed == len ( splitted_md_rows ): raise StepFailed ( f \"all { len ( splitted_md_rows ) } embeddings got skipped\" ) return DataFrame [ EmbeddingResult ]( DataFrame [ EmbeddingResult ]( rows )) def get_embedding_input_from_document ( self , doc : MarkdownDataContract ) -> str : \"\"\"Clean the document such that it can be used as input to the embedding model. Parameters ---------- doc : MarkdownDataContract The document containing the page content in Markdown format. Returns ------- str Cleaned text that can be used as input to the embedding model. \"\"\" plain_text = self . markdown . convert ( doc . md ) plain_text = self . _replace_link ( plain_text ) return plain_text def _get_embedding ( self , doc : MarkdownDataContract ) -> Embedded : \"\"\"Generates an embedding for a given text and context. Parameters ---------- d : dict A dictionary containing the text and context for which to generate the embedding. Returns ------- dict A dictionary containing the original text, its embedding, and the source URL. \"\"\" context = self . get_simple_context ( doc . keywords ) text = self . get_embedding_input_from_document ( doc ) if self . settings . CLEAN_MD_BEFORE_EMBEDDING else doc . md vector = self . embedding . embed_query ( text ) return { \"text\" : doc . md , \"vector\" : vector , \"url\" : doc . url , \"keywords\" : context } def is_stopword ( self , word : str ) -> bool : \"\"\"Stopword Detection Function.\"\"\" return word . lower () in self . settingstopwords @classmethod def whitespace_word_tokenizer ( cls , text : str ) -> list [ str ]: \"\"\"Simple Regex based whitespace word tokenizer.\"\"\" return [ x for x in re . split ( r \"([.,!?]+)?\\s+\" , text ) if x ] def get_simple_context ( self , text ): \"\"\"Simple function to create a context from a text.\"\"\" tokens = self . whitespace_word_tokenizer ( text ) filtered_tokens = [ token for token in tokens if not self . is_stopword ( token )] return \" \" . join ( filtered_tokens ) @classmethod def __md_to_plain ( cls , element , stream : Optional [ StringIO ] = None ): \"\"\"Converts a markdown element into plain text. Parameters ---------- element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created. Returns ------- str The plain text representation of the markdown element. \"\"\" if stream is None : stream = StringIO () if element . text : stream . write ( element . text ) for sub in element : cls . __md_to_plain ( sub , stream ) if element . tail : stream . write ( element . tail ) return stream . getvalue () @classmethod def _replace_link ( cls , text : str ): \"\"\"Replaces URLs in the text with a placeholder. Parameters ---------- text : str The text in which URLs will be replaced. Returns ------- str The text with URLs replaced by 'LINK'. \"\"\" # Extract URL from a string url_extract_pattern = ( \"https?: \\\\ / \\\\ /(?:www \\\\ .)?[-a-zA-Z0-9@:%._ \\\\ +~#=]{1,256} \\\\ .[a-zA-Z0-9()]{1,6} \\\\ b(?:[-a-zA-Z0-9()@:%_ \\\\ +.~#?& \\\\ /=]*)\" # pylint: disable=line-too-long ) links = sorted ( re . findall ( url_extract_pattern , text ), key = len , reverse = True ) for link in links : text = text . replace ( link , \"LINK\" ) return text __md_to_plain ( element , stream = None ) classmethod Converts a markdown element into plain text. Parameters element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created. Returns str The plain text representation of the markdown element. Source code in wurzel/steps/embedding/step.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @classmethod def __md_to_plain ( cls , element , stream : Optional [ StringIO ] = None ): \"\"\"Converts a markdown element into plain text. Parameters ---------- element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created. Returns ------- str The plain text representation of the markdown element. \"\"\" if stream is None : stream = StringIO () if element . text : stream . write ( element . text ) for sub in element : cls . __md_to_plain ( sub , stream ) if element . tail : stream . write ( element . tail ) return stream . getvalue () get_embedding_input_from_document ( doc ) Clean the document such that it can be used as input to the embedding model. Parameters doc : MarkdownDataContract The document containing the page content in Markdown format. Returns str Cleaned text that can be used as input to the embedding model. Source code in wurzel/steps/embedding/step.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_embedding_input_from_document ( self , doc : MarkdownDataContract ) -> str : \"\"\"Clean the document such that it can be used as input to the embedding model. Parameters ---------- doc : MarkdownDataContract The document containing the page content in Markdown format. Returns ------- str Cleaned text that can be used as input to the embedding model. \"\"\" plain_text = self . markdown . convert ( doc . md ) plain_text = self . _replace_link ( plain_text ) return plain_text get_simple_context ( text ) Simple function to create a context from a text. Source code in wurzel/steps/embedding/step.py 160 161 162 163 164 def get_simple_context ( self , text ): \"\"\"Simple function to create a context from a text.\"\"\" tokens = self . whitespace_word_tokenizer ( text ) filtered_tokens = [ token for token in tokens if not self . is_stopword ( token )] return \" \" . join ( filtered_tokens ) is_stopword ( word ) Stopword Detection Function. Source code in wurzel/steps/embedding/step.py 151 152 153 def is_stopword ( self , word : str ) -> bool : \"\"\"Stopword Detection Function.\"\"\" return word . lower () in self . settingstopwords run ( inpt ) Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. Source code in wurzel/steps/embedding/step.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingResult ]: \"\"\"Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. \"\"\" if len ( inpt ) == 0 : log . info ( \"Got empty result in Embedding - Skipping\" ) return DataFrame [ EmbeddingResult ]([]) splitted_md_rows = self . _split_markdown ( inpt ) rows = [] failed = 0 for row in tqdm ( splitted_md_rows , desc = \"Calculate Embeddings\" ): try : rows . append ( self . _get_embedding ( row )) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( row )}, ) failed += 1 if failed : log . warning ( f \" { failed } / { len ( splitted_md_rows ) } got skipped\" ) if failed == len ( splitted_md_rows ): raise StepFailed ( f \"all { len ( splitted_md_rows ) } embeddings got skipped\" ) return DataFrame [ EmbeddingResult ]( DataFrame [ EmbeddingResult ]( rows )) whitespace_word_tokenizer ( text ) classmethod Simple Regex based whitespace word tokenizer. Source code in wurzel/steps/embedding/step.py 155 156 157 158 @classmethod def whitespace_word_tokenizer ( cls , text : str ) -> list [ str ]: \"\"\"Simple Regex based whitespace word tokenizer.\"\"\" return [ x for x in re . split ( r \"([.,!?]+)?\\s+\" , text ) if x ] consists of DVCSteps to embedd files and save them as for example as csv. EmbeddingMultiVectorStep Bases: EmbeddingStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingMultiVectorResult ]] Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult]. Source code in wurzel/steps/embedding/step_multivector.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class EmbeddingMultiVectorStep ( EmbeddingStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingMultiVectorResult ], ], ): \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult]. \"\"\" def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingMultiVectorResult ]: \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Args: inpt (list[MarkdownDataContract]): A list of markdown data contracts to process. Returns: DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed: If all input documents fail to generate embeddings. Logs: - Warnings for documents skipped due to EmbeddingAPIException. - A summary warning if some or all documents are skipped. \"\"\" def process_document ( doc ): try : return self . _get_embedding ( doc ) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( doc )}, ) return None results = Parallel ( backend = \"threading\" , n_jobs = self . settings . N_JOBS )( delayed ( process_document )( doc ) for doc in inpt ) rows = [ res for res in results if res is not None ] failed = len ( results ) - len ( rows ) if failed : log . warning ( f \" { failed } / { len ( inpt ) } got skipped\" ) if failed == len ( inpt ): raise StepFailed ( f \"All { len ( inpt ) } embeddings got skipped\" ) return DataFrame [ EmbeddingMultiVectorResult ]( DataFrame [ EmbeddingMultiVectorResult ]( rows )) def _get_embedding ( self , doc : MarkdownDataContract ) -> _EmbeddedMultiVector : \"\"\"Generates an embedding for a given text and context. Parameters ---------- d : dict A dictionary containing the text and context for which to generate the embedding. Returns ------- dict A dictionary containing the original text, its embedding, and the source URL. \"\"\" def prepare_plain ( document : MarkdownDataContract ) -> str : plain_text = self . markdown . convert ( document . md ) plain_text = self . _replace_link ( plain_text ) return plain_text try : splitted_md_rows = self . _split_markdown ([ doc ]) except SplittException as err : raise EmbeddingAPIException ( \"splitting failed\" ) from err vectors = [ self . embedding . embed_query ( prepare_plain ( split )) for split in splitted_md_rows ] if not vectors : raise EmbeddingAPIException ( \"Embedding failed for all splits\" ) context = self . get_simple_context ( doc . keywords ) return { \"text\" : doc . md , \"vectors\" : vectors , \"url\" : doc . url , \"keywords\" : context , \"splits\" : [ split . md for split in splitted_md_rows ], } run ( inpt ) Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Parameters: inpt ( list [ MarkdownDataContract ] ) \u2013 A list of markdown data contracts to process. Returns: DataFrame [ EmbeddingMultiVectorResult ] \u2013 DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed \u2013 If all input documents fail to generate embeddings. Logs Warnings for documents skipped due to EmbeddingAPIException. A summary warning if some or all documents are skipped. Source code in wurzel/steps/embedding/step_multivector.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingMultiVectorResult ]: \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Args: inpt (list[MarkdownDataContract]): A list of markdown data contracts to process. Returns: DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed: If all input documents fail to generate embeddings. Logs: - Warnings for documents skipped due to EmbeddingAPIException. - A summary warning if some or all documents are skipped. \"\"\" def process_document ( doc ): try : return self . _get_embedding ( doc ) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( doc )}, ) return None results = Parallel ( backend = \"threading\" , n_jobs = self . settings . N_JOBS )( delayed ( process_document )( doc ) for doc in inpt ) rows = [ res for res in results if res is not None ] failed = len ( results ) - len ( rows ) if failed : log . warning ( f \" { failed } / { len ( inpt ) } got skipped\" ) if failed == len ( inpt ): raise StepFailed ( f \"All { len ( inpt ) } embeddings got skipped\" ) return DataFrame [ EmbeddingMultiVectorResult ]( DataFrame [ EmbeddingMultiVectorResult ]( rows )) EmbeddingSettings Bases: SplitterSettings EmbeddingSettings is a configuration class for embedding-related settings. Attributes: API ( Url ) \u2013 The API endpoint for embedding operations. NORMALIZE ( bool ) \u2013 A flag indicating whether to normalize embeddings. Defaults to False. BATCH_SIZE ( int ) \u2013 The batch size for processing embeddings. Must be greater than 0. Defaults to 100. TOKEN_COUNT_MIN ( int ) \u2013 The minimum token count for processing. Must be greater than 0. Defaults to 64. TOKEN_COUNT_MAX ( int ) \u2013 The maximum token count for processing. Must be greater than 1. Defaults to 256. TOKEN_COUNT_BUFFER ( int ) \u2013 The buffer size for token count. Must be greater than 0. Defaults to 32. STEPWORDS_PATH ( Path ) \u2013 The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\". N_JOBS ( int ) \u2013 The number of parallel jobs to use. Must be greater than 0. Defaults to 1. PREFIX_MAP ( dict [ Pattern , str ] ) \u2013 A mapping of regex patterns to string prefixes. This is validated and transformed using the _wrap_validator_model_mapping method. Methods: Name Description _wrap_validator_model_mapping dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result. Source code in wurzel/steps/embedding/settings.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class EmbeddingSettings ( SplitterSettings ): \"\"\"EmbeddingSettings is a configuration class for embedding-related settings. Attributes: API (Url): The API endpoint for embedding operations. NORMALIZE (bool): A flag indicating whether to normalize embeddings. Defaults to False. BATCH_SIZE (int): The batch size for processing embeddings. Must be greater than 0. Defaults to 100. TOKEN_COUNT_MIN (int): The minimum token count for processing. Must be greater than 0. Defaults to 64. TOKEN_COUNT_MAX (int): The maximum token count for processing. Must be greater than 1. Defaults to 256. TOKEN_COUNT_BUFFER (int): The buffer size for token count. Must be greater than 0. Defaults to 32. STEPWORDS_PATH (Path): The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\". N_JOBS (int): The number of parallel jobs to use. Must be greater than 0. Defaults to 1. PREFIX_MAP (dict[re.Pattern, str]): A mapping of regex patterns to string prefixes. This is validated and transformed using the `_wrap_validator_model_mapping` method. Methods: _wrap_validator_model_mapping(input_dict: dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result. \"\"\" @staticmethod def _wrap_validator_model_mapping ( input_dict : dict [ str , str ], handler ): new_dict = {} for regex , prefix in input_dict . items (): if isinstance ( regex , str ): new_dict [ re . compile ( regex )] = prefix else : new_dict . update ({ regex : prefix }) return handler ( new_dict ) API : Url NORMALIZE : bool = False BATCH_SIZE : int = Field ( 100 , gt = 0 ) TOKEN_COUNT_MIN : int = Field ( 64 , gt = 0 ) TOKEN_COUNT_MAX : int = Field ( 256 , gt = 1 ) TOKEN_COUNT_BUFFER : int = Field ( 32 , gt = 0 ) STEPWORDS_PATH : Path = Path ( \"data/german_stopwords_full.txt\" ) N_JOBS : int = Field ( 1 , gt = 0 ) PREFIX_MAP : Annotated [ dict [ re . Pattern , str ], WrapValidator ( _wrap_validator_model_mapping )] = Field ( default = { \"e5-\" : \"query: \" , \"DPR|dpr\" : \"\" } ) CLEAN_MD_BEFORE_EMBEDDING : bool = True","title":"Embedding"},{"location":"steps/embedding/#wurzel.steps.embedding.step.Embedded","text":"Bases: TypedDict dict definition of a embedded document. Source code in wurzel/steps/embedding/step.py 33 34 35 36 37 38 class Embedded ( TypedDict ): \"\"\"dict definition of a embedded document.\"\"\" text : str url : str vector : list [ float ]","title":"Embedded"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep","text":"Bases: SimpleSplitterStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingResult ]] Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult]. Source code in wurzel/steps/embedding/step.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class EmbeddingStep ( SimpleSplitterStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingResult ]], ): \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingResult]. \"\"\" embedding : HuggingFaceInferenceAPIEmbeddings n_jobs : int markdown : Markdown stopwords : list [ str ] settings : EmbeddingSettings def __init__ ( self ) -> None : super () . __init__ () self . settings = EmbeddingSettings () self . embedding = self . _select_embedding () self . n_jobs = max ( 1 , ( os . cpu_count () or 0 ) - 1 ) # Inject net output_format into 3rd party library Markdown Markdown . output_formats [ \"plain\" ] = self . __md_to_plain # type: ignore[index] self . markdown = Markdown ( output_format = \"plain\" ) # type: ignore[arg-type] self . markdown . stripTopLevelTags = False self . settingstopwords = self . _load_stopwords () self . splitter = SemanticSplitter ( token_limit = self . settings . TOKEN_COUNT_MAX , token_limit_buffer = self . settings . TOKEN_COUNT_BUFFER , token_limit_min = self . settings . TOKEN_COUNT_MIN , ) def _load_stopwords ( self ) -> list [ str ]: path = self . settings . STEPWORDS_PATH with path . open ( encoding = \"utf-8\" ) as f : stopwords = [ w . strip () for w in f . readlines () if not w . startswith ( \";\" )] return stopwords def _select_embedding ( self ) -> HuggingFaceInferenceAPIEmbeddings : \"\"\"Selects the embedding model to be used for generating embeddings. Returns ------- Embeddings An instance of the Embeddings class. \"\"\" return PrefixedAPIEmbeddings ( self . settings . API , self . settings . PREFIX_MAP ) def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingResult ]: \"\"\"Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. \"\"\" if len ( inpt ) == 0 : log . info ( \"Got empty result in Embedding - Skipping\" ) return DataFrame [ EmbeddingResult ]([]) splitted_md_rows = self . _split_markdown ( inpt ) rows = [] failed = 0 for row in tqdm ( splitted_md_rows , desc = \"Calculate Embeddings\" ): try : rows . append ( self . _get_embedding ( row )) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( row )}, ) failed += 1 if failed : log . warning ( f \" { failed } / { len ( splitted_md_rows ) } got skipped\" ) if failed == len ( splitted_md_rows ): raise StepFailed ( f \"all { len ( splitted_md_rows ) } embeddings got skipped\" ) return DataFrame [ EmbeddingResult ]( DataFrame [ EmbeddingResult ]( rows )) def get_embedding_input_from_document ( self , doc : MarkdownDataContract ) -> str : \"\"\"Clean the document such that it can be used as input to the embedding model. Parameters ---------- doc : MarkdownDataContract The document containing the page content in Markdown format. Returns ------- str Cleaned text that can be used as input to the embedding model. \"\"\" plain_text = self . markdown . convert ( doc . md ) plain_text = self . _replace_link ( plain_text ) return plain_text def _get_embedding ( self , doc : MarkdownDataContract ) -> Embedded : \"\"\"Generates an embedding for a given text and context. Parameters ---------- d : dict A dictionary containing the text and context for which to generate the embedding. Returns ------- dict A dictionary containing the original text, its embedding, and the source URL. \"\"\" context = self . get_simple_context ( doc . keywords ) text = self . get_embedding_input_from_document ( doc ) if self . settings . CLEAN_MD_BEFORE_EMBEDDING else doc . md vector = self . embedding . embed_query ( text ) return { \"text\" : doc . md , \"vector\" : vector , \"url\" : doc . url , \"keywords\" : context } def is_stopword ( self , word : str ) -> bool : \"\"\"Stopword Detection Function.\"\"\" return word . lower () in self . settingstopwords @classmethod def whitespace_word_tokenizer ( cls , text : str ) -> list [ str ]: \"\"\"Simple Regex based whitespace word tokenizer.\"\"\" return [ x for x in re . split ( r \"([.,!?]+)?\\s+\" , text ) if x ] def get_simple_context ( self , text ): \"\"\"Simple function to create a context from a text.\"\"\" tokens = self . whitespace_word_tokenizer ( text ) filtered_tokens = [ token for token in tokens if not self . is_stopword ( token )] return \" \" . join ( filtered_tokens ) @classmethod def __md_to_plain ( cls , element , stream : Optional [ StringIO ] = None ): \"\"\"Converts a markdown element into plain text. Parameters ---------- element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created. Returns ------- str The plain text representation of the markdown element. \"\"\" if stream is None : stream = StringIO () if element . text : stream . write ( element . text ) for sub in element : cls . __md_to_plain ( sub , stream ) if element . tail : stream . write ( element . tail ) return stream . getvalue () @classmethod def _replace_link ( cls , text : str ): \"\"\"Replaces URLs in the text with a placeholder. Parameters ---------- text : str The text in which URLs will be replaced. Returns ------- str The text with URLs replaced by 'LINK'. \"\"\" # Extract URL from a string url_extract_pattern = ( \"https?: \\\\ / \\\\ /(?:www \\\\ .)?[-a-zA-Z0-9@:%._ \\\\ +~#=]{1,256} \\\\ .[a-zA-Z0-9()]{1,6} \\\\ b(?:[-a-zA-Z0-9()@:%_ \\\\ +.~#?& \\\\ /=]*)\" # pylint: disable=line-too-long ) links = sorted ( re . findall ( url_extract_pattern , text ), key = len , reverse = True ) for link in links : text = text . replace ( link , \"LINK\" ) return text","title":"EmbeddingStep"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain","text":"Converts a markdown element into plain text.","title":"__md_to_plain"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain--parameters","text":"element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created.","title":"Parameters"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.__md_to_plain--returns","text":"str The plain text representation of the markdown element. Source code in wurzel/steps/embedding/step.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @classmethod def __md_to_plain ( cls , element , stream : Optional [ StringIO ] = None ): \"\"\"Converts a markdown element into plain text. Parameters ---------- element : Element The markdown element to convert. stream : StringIO, optional The stream to which the plain text is written. If None, a new stream is created. Returns ------- str The plain text representation of the markdown element. \"\"\" if stream is None : stream = StringIO () if element . text : stream . write ( element . text ) for sub in element : cls . __md_to_plain ( sub , stream ) if element . tail : stream . write ( element . tail ) return stream . getvalue ()","title":"Returns"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document","text":"Clean the document such that it can be used as input to the embedding model.","title":"get_embedding_input_from_document"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document--parameters","text":"doc : MarkdownDataContract The document containing the page content in Markdown format.","title":"Parameters"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_embedding_input_from_document--returns","text":"str Cleaned text that can be used as input to the embedding model. Source code in wurzel/steps/embedding/step.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_embedding_input_from_document ( self , doc : MarkdownDataContract ) -> str : \"\"\"Clean the document such that it can be used as input to the embedding model. Parameters ---------- doc : MarkdownDataContract The document containing the page content in Markdown format. Returns ------- str Cleaned text that can be used as input to the embedding model. \"\"\" plain_text = self . markdown . convert ( doc . md ) plain_text = self . _replace_link ( plain_text ) return plain_text","title":"Returns"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.get_simple_context","text":"Simple function to create a context from a text. Source code in wurzel/steps/embedding/step.py 160 161 162 163 164 def get_simple_context ( self , text ): \"\"\"Simple function to create a context from a text.\"\"\" tokens = self . whitespace_word_tokenizer ( text ) filtered_tokens = [ token for token in tokens if not self . is_stopword ( token )] return \" \" . join ( filtered_tokens )","title":"get_simple_context"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.is_stopword","text":"Stopword Detection Function. Source code in wurzel/steps/embedding/step.py 151 152 153 def is_stopword ( self , word : str ) -> bool : \"\"\"Stopword Detection Function.\"\"\" return word . lower () in self . settingstopwords","title":"is_stopword"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.run","text":"Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. Source code in wurzel/steps/embedding/step.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingResult ]: \"\"\"Executes the embedding step by processing input markdown files, generating embeddings, and saving them to a CSV file. \"\"\" if len ( inpt ) == 0 : log . info ( \"Got empty result in Embedding - Skipping\" ) return DataFrame [ EmbeddingResult ]([]) splitted_md_rows = self . _split_markdown ( inpt ) rows = [] failed = 0 for row in tqdm ( splitted_md_rows , desc = \"Calculate Embeddings\" ): try : rows . append ( self . _get_embedding ( row )) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( row )}, ) failed += 1 if failed : log . warning ( f \" { failed } / { len ( splitted_md_rows ) } got skipped\" ) if failed == len ( splitted_md_rows ): raise StepFailed ( f \"all { len ( splitted_md_rows ) } embeddings got skipped\" ) return DataFrame [ EmbeddingResult ]( DataFrame [ EmbeddingResult ]( rows ))","title":"run"},{"location":"steps/embedding/#wurzel.steps.embedding.step.EmbeddingStep.whitespace_word_tokenizer","text":"Simple Regex based whitespace word tokenizer. Source code in wurzel/steps/embedding/step.py 155 156 157 158 @classmethod def whitespace_word_tokenizer ( cls , text : str ) -> list [ str ]: \"\"\"Simple Regex based whitespace word tokenizer.\"\"\" return [ x for x in re . split ( r \"([.,!?]+)?\\s+\" , text ) if x ] consists of DVCSteps to embedd files and save them as for example as csv.","title":"whitespace_word_tokenizer"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep","text":"Bases: EmbeddingStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingMultiVectorResult ]] Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult]. Source code in wurzel/steps/embedding/step_multivector.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class EmbeddingMultiVectorStep ( EmbeddingStep , TypedStep [ EmbeddingSettings , list [ MarkdownDataContract ], DataFrame [ EmbeddingMultiVectorResult ], ], ): \"\"\"Step for consuming list[MarkdownDataContract] and returning DataFrame[EmbeddingMultiVectorResult]. \"\"\" def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingMultiVectorResult ]: \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Args: inpt (list[MarkdownDataContract]): A list of markdown data contracts to process. Returns: DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed: If all input documents fail to generate embeddings. Logs: - Warnings for documents skipped due to EmbeddingAPIException. - A summary warning if some or all documents are skipped. \"\"\" def process_document ( doc ): try : return self . _get_embedding ( doc ) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( doc )}, ) return None results = Parallel ( backend = \"threading\" , n_jobs = self . settings . N_JOBS )( delayed ( process_document )( doc ) for doc in inpt ) rows = [ res for res in results if res is not None ] failed = len ( results ) - len ( rows ) if failed : log . warning ( f \" { failed } / { len ( inpt ) } got skipped\" ) if failed == len ( inpt ): raise StepFailed ( f \"All { len ( inpt ) } embeddings got skipped\" ) return DataFrame [ EmbeddingMultiVectorResult ]( DataFrame [ EmbeddingMultiVectorResult ]( rows )) def _get_embedding ( self , doc : MarkdownDataContract ) -> _EmbeddedMultiVector : \"\"\"Generates an embedding for a given text and context. Parameters ---------- d : dict A dictionary containing the text and context for which to generate the embedding. Returns ------- dict A dictionary containing the original text, its embedding, and the source URL. \"\"\" def prepare_plain ( document : MarkdownDataContract ) -> str : plain_text = self . markdown . convert ( document . md ) plain_text = self . _replace_link ( plain_text ) return plain_text try : splitted_md_rows = self . _split_markdown ([ doc ]) except SplittException as err : raise EmbeddingAPIException ( \"splitting failed\" ) from err vectors = [ self . embedding . embed_query ( prepare_plain ( split )) for split in splitted_md_rows ] if not vectors : raise EmbeddingAPIException ( \"Embedding failed for all splits\" ) context = self . get_simple_context ( doc . keywords ) return { \"text\" : doc . md , \"vectors\" : vectors , \"url\" : doc . url , \"keywords\" : context , \"splits\" : [ split . md for split in splitted_md_rows ], }","title":"EmbeddingMultiVectorStep"},{"location":"steps/embedding/#wurzel.steps.embedding.step_multivector.EmbeddingMultiVectorStep.run","text":"Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Parameters: inpt ( list [ MarkdownDataContract ] ) \u2013 A list of markdown data contracts to process. Returns: DataFrame [ EmbeddingMultiVectorResult ] \u2013 DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed \u2013 If all input documents fail to generate embeddings. Logs Warnings for documents skipped due to EmbeddingAPIException. A summary warning if some or all documents are skipped. Source code in wurzel/steps/embedding/step_multivector.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def run ( self , inpt : list [ MarkdownDataContract ]) -> DataFrame [ EmbeddingMultiVectorResult ]: \"\"\"Executes the embedding step by processing a list of MarkdownDataContract objects, generating embeddings for each document, and returning the results as a DataFrame. Args: inpt (list[MarkdownDataContract]): A list of markdown data contracts to process. Returns: DataFrame[EmbeddingMultiVectorResult]: A DataFrame containing the embedding results. Raises: StepFailed: If all input documents fail to generate embeddings. Logs: - Warnings for documents skipped due to EmbeddingAPIException. - A summary warning if some or all documents are skipped. \"\"\" def process_document ( doc ): try : return self . _get_embedding ( doc ) except EmbeddingAPIException as err : log . warning ( f \"Skipped because EmbeddingAPIException: { err . message } \" , extra = { \"markdown\" : str ( doc )}, ) return None results = Parallel ( backend = \"threading\" , n_jobs = self . settings . N_JOBS )( delayed ( process_document )( doc ) for doc in inpt ) rows = [ res for res in results if res is not None ] failed = len ( results ) - len ( rows ) if failed : log . warning ( f \" { failed } / { len ( inpt ) } got skipped\" ) if failed == len ( inpt ): raise StepFailed ( f \"All { len ( inpt ) } embeddings got skipped\" ) return DataFrame [ EmbeddingMultiVectorResult ]( DataFrame [ EmbeddingMultiVectorResult ]( rows ))","title":"run"},{"location":"steps/embedding/#wurzel.steps.embedding.settings.EmbeddingSettings","text":"Bases: SplitterSettings EmbeddingSettings is a configuration class for embedding-related settings. Attributes: API ( Url ) \u2013 The API endpoint for embedding operations. NORMALIZE ( bool ) \u2013 A flag indicating whether to normalize embeddings. Defaults to False. BATCH_SIZE ( int ) \u2013 The batch size for processing embeddings. Must be greater than 0. Defaults to 100. TOKEN_COUNT_MIN ( int ) \u2013 The minimum token count for processing. Must be greater than 0. Defaults to 64. TOKEN_COUNT_MAX ( int ) \u2013 The maximum token count for processing. Must be greater than 1. Defaults to 256. TOKEN_COUNT_BUFFER ( int ) \u2013 The buffer size for token count. Must be greater than 0. Defaults to 32. STEPWORDS_PATH ( Path ) \u2013 The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\". N_JOBS ( int ) \u2013 The number of parallel jobs to use. Must be greater than 0. Defaults to 1. PREFIX_MAP ( dict [ Pattern , str ] ) \u2013 A mapping of regex patterns to string prefixes. This is validated and transformed using the _wrap_validator_model_mapping method. Methods: Name Description _wrap_validator_model_mapping dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result. Source code in wurzel/steps/embedding/settings.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class EmbeddingSettings ( SplitterSettings ): \"\"\"EmbeddingSettings is a configuration class for embedding-related settings. Attributes: API (Url): The API endpoint for embedding operations. NORMALIZE (bool): A flag indicating whether to normalize embeddings. Defaults to False. BATCH_SIZE (int): The batch size for processing embeddings. Must be greater than 0. Defaults to 100. TOKEN_COUNT_MIN (int): The minimum token count for processing. Must be greater than 0. Defaults to 64. TOKEN_COUNT_MAX (int): The maximum token count for processing. Must be greater than 1. Defaults to 256. TOKEN_COUNT_BUFFER (int): The buffer size for token count. Must be greater than 0. Defaults to 32. STEPWORDS_PATH (Path): The file path to the stopwords file. Defaults to \"data/german_stopwords_full.txt\". N_JOBS (int): The number of parallel jobs to use. Must be greater than 0. Defaults to 1. PREFIX_MAP (dict[re.Pattern, str]): A mapping of regex patterns to string prefixes. This is validated and transformed using the `_wrap_validator_model_mapping` method. Methods: _wrap_validator_model_mapping(input_dict: dict[str, str], handler): A static method to wrap and validate the model mapping. It converts string regex keys in the input dictionary to compiled regex patterns and applies a handler function to the result. \"\"\" @staticmethod def _wrap_validator_model_mapping ( input_dict : dict [ str , str ], handler ): new_dict = {} for regex , prefix in input_dict . items (): if isinstance ( regex , str ): new_dict [ re . compile ( regex )] = prefix else : new_dict . update ({ regex : prefix }) return handler ( new_dict ) API : Url NORMALIZE : bool = False BATCH_SIZE : int = Field ( 100 , gt = 0 ) TOKEN_COUNT_MIN : int = Field ( 64 , gt = 0 ) TOKEN_COUNT_MAX : int = Field ( 256 , gt = 1 ) TOKEN_COUNT_BUFFER : int = Field ( 32 , gt = 0 ) STEPWORDS_PATH : Path = Path ( \"data/german_stopwords_full.txt\" ) N_JOBS : int = Field ( 1 , gt = 0 ) PREFIX_MAP : Annotated [ dict [ re . Pattern , str ], WrapValidator ( _wrap_validator_model_mapping )] = Field ( default = { \"e5-\" : \"query: \" , \"DPR|dpr\" : \"\" } ) CLEAN_MD_BEFORE_EMBEDDING : bool = True","title":"EmbeddingSettings"},{"location":"steps/milvus/","text":"containing the DVCStep sending embedding data into milvus. MilvusConnectorStep Bases: TypedStep [ MilvusSettings , DataFrame [ EmbeddingResult ], Result ] Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/milvus/step.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class MilvusConnectorStep ( TypedStep [ MilvusSettings , DataFrame [ EmbeddingResult ], MilvusResult ]): # pragma: no cover \"\"\"Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" milvus_timeout : float = 20.0 def __init__ ( self ) -> None : super () . __init__ () # milvus stuff passed as environment # because we need to enject them into the DVC step during runtime, # not during DVC pipeline definition time uri = f \"http:// { self . settings . HOST } : { self . settings . PORT } \" if not self . settings . PASSWORD or not self . settings . USER : log . warning ( \"MILVUS_HOST, MILVUS_USER or MILVUS_PASSWORD for Milvus not provided. Thus running in non-credential Mode\" ) self . client : MilvusClient = MilvusClient ( uri = uri , user = self . settings . USER , password = self . settings . PASSWORD , timeout = self . milvus_timeout , ) self . collection_index : IndexParams = IndexParams ( ** self . settings . INDEX_PARAMS ) self . collection_history_len = self . settings . COLLECTION_HISTORY_LEN self . collection_prefix = self . settings . COLLECTION def __del__ ( self ): if getattr ( self , \"client\" , None ): self . client . close () def run ( self , inpt : DataFrame [ EmbeddingResult ]) -> MilvusResult : self . _insert_embeddings ( inpt ) try : old = self . __construct_last_collection_name () except NoPreviousCollection : old = \"\" self . _retire_collection () return MilvusResult ( new = self . __construct_current_collection_name (), old = old ) def _insert_embeddings ( self , data : pd . DataFrame ): collection_name = self . __construct_next_collection_name () log . info ( f \"Creating milvus collection { collection_name } \" ) collection_schema = CollectionSchema ( fields = [ FieldSchema ( name = \"pk\" , dtype = DataType . INT64 , is_primary = True , auto_id = True ), FieldSchema ( name = \"text\" , dtype = DataType . VARCHAR , max_length = 3000 ), FieldSchema ( name = \"vector\" , dtype = DataType . FLOAT_VECTOR , dim = len ( data [ \"vector\" ] . loc [ 0 ]), ), FieldSchema ( name = \"url\" , dtype = DataType . VARCHAR , max_length = 300 ), ], description = \"Collection for storing Milvus embeddings\" , ) log . info ( \"schema created\" ) self . client . create_collection ( collection_name = collection_name , schema = collection_schema ) log . info ( \"collection created\" ) log . info ( f \"Inserting embedding { len ( data ) } into collection { collection_name } \" ) result : dict = self . client . insert ( collection_name = collection_name , data = data . to_dict ( \"records\" )) if result [ \"insert_count\" ] != len ( data ): raise StepFailed ( f \"Failed to insert df into collection ' { collection_name } '. { result [ 'insert_count' ] } / { len ( data ) } where successful\" ) log . info ( f \"Successfully inserted { len ( data ) } vectors into collection ' { collection_name } '\" ) self . client . create_index ( collection_name = collection_name , index_params = self . collection_index ) log . info ( f \"Successfully craeted index { self . collection_index } into collection ' { collection_name } \" ) self . client . load_collection ( collection_name ) log . info ( f \"Successfully loaded the collection { collection_name } ' into collection ' { collection_name } '\" ) try : self . client . release_collection ( self . __construct_last_collection_name ()) except NoPreviousCollection : pass self . _update_alias ( collection_name ) def _retire_collection ( self ): collections_versioned : dict [ int , str ] = self . _get_collection_versions () to_delete = sorted ( collections_versioned . keys ())[: - self . collection_history_len ] if not to_delete : return for col_v in to_delete : col = collections_versioned [ col_v ] log . info ( f \"deleting { col } collection caused by retirement\" ) self . client . drop_collection ( col , timeout = self . milvus_timeout ) def _update_alias ( self , collection_name ): try : self . client . create_alias ( collection_name = collection_name , alias = self . collection_prefix , timeout = self . milvus_timeout , ) except MilvusException : self . client . alter_alias ( collection_name = collection_name , alias = self . collection_prefix , timeout = self . milvus_timeout , ) def __construct_next_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections : return f \" { self . collection_prefix } _v1\" previous_version = max ( previous_collections . keys ()) log . info ( f \"Found version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version + 1 } \" def __construct_last_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections or len ( previous_collections ) <= 1 : raise NoPreviousCollection ( f \"Milvus does not contain a previous collection for { self . collection_prefix } \" ) previous_version = sorted ( previous_collections . keys ())[ - 2 ] log . info ( f \"Found previous version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version } \" def __construct_current_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections or len ( previous_collections ) < 1 : raise NoPreviousCollection ( f \"Milvus does not contain a previous collection for { self . collection_prefix } \" ) previous_version = sorted ( previous_collections . keys ())[ - 1 ] log . info ( f \"Found previous version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version } \" def _get_collection_versions ( self ) -> dict [ int , str ]: previous_collections = self . client . list_collections ( timeout = self . milvus_timeout ) versioned_collections = { int ( previous . split ( \"_v\" )[ - 1 ]): previous for previous in previous_collections if self . collection_prefix in previous } return versioned_collections MilvusSettings Bases: Settings MilvusSettings is a configuration class for managing settings related to MilvusDB. Attributes: HOST ( str ) \u2013 The hostname or IP address of the Milvus server. Defaults to \"localhost\". PORT ( int ) \u2013 The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530. COLLECTION ( str ) \u2013 The name of the collection in MilvusDB. COLLECTION_HISTORY_LEN ( int ) \u2013 The length of the collection history. Defaults to 10. SEARCH_PARAMS ( dict ) \u2013 Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS ( dict ) \u2013 Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\", \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}. USER ( str ) \u2013 The username for authentication with MilvusDB. PASSWORD ( str ) \u2013 The password for authentication with MilvusDB. SECURED ( bool ) \u2013 Indicates whether the connection to MilvusDB is secured. Defaults to False. Methods: Name Description parse_json Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Parameters: v ( str or dict ) \u2013 The value to validate and parse. Returns: dict \u2013 The parsed dictionary if the input is a JSON string, or the original value if it is already a dictionary. Source code in wurzel/steps/milvus/settings.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class MilvusSettings ( Settings ): \"\"\"MilvusSettings is a configuration class for managing settings related to MilvusDB. Attributes: HOST (str): The hostname or IP address of the Milvus server. Defaults to \"localhost\". PORT (int): The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530. COLLECTION (str): The name of the collection in MilvusDB. COLLECTION_HISTORY_LEN (int): The length of the collection history. Defaults to 10. SEARCH_PARAMS (dict): Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS (dict): Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\", \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}. USER (str): The username for authentication with MilvusDB. PASSWORD (str): The password for authentication with MilvusDB. SECURED (bool): Indicates whether the connection to MilvusDB is secured. Defaults to False. Methods: parse_json(cls, v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Args: v (str or dict): The value to validate and parse. Returns: dict: The parsed dictionary if the input is a JSON string, or the original value if it is already a dictionary. \"\"\" HOST : str = \"localhost\" PORT : int = Field ( 19530 , gt = 0 , le = 65535 ) COLLECTION : str COLLECTION_HISTORY_LEN : int = 10 SEARCH_PARAMS : dict = { \"metric_type\" : \"IP\" , \"params\" : {}} INDEX_PARAMS : dict = { \"index_type\" : \"FLAT\" , \"field_name\" : \"vector\" , \"metric_type\" : \"IP\" , \"params\" : {}, } USER : str PASSWORD : str SECURED : bool = False @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod # pylint: disable-next=R0801 def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v parse_json ( v ) classmethod Validation for json. Source code in wurzel/steps/milvus/settings.py 54 55 56 57 58 59 60 61 @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod # pylint: disable-next=R0801 def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"Milvus"},{"location":"steps/milvus/#wurzel.steps.milvus.step.MilvusConnectorStep","text":"Bases: TypedStep [ MilvusSettings , DataFrame [ EmbeddingResult ], Result ] Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/milvus/step.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class MilvusConnectorStep ( TypedStep [ MilvusSettings , DataFrame [ EmbeddingResult ], MilvusResult ]): # pragma: no cover \"\"\"Milvus connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" milvus_timeout : float = 20.0 def __init__ ( self ) -> None : super () . __init__ () # milvus stuff passed as environment # because we need to enject them into the DVC step during runtime, # not during DVC pipeline definition time uri = f \"http:// { self . settings . HOST } : { self . settings . PORT } \" if not self . settings . PASSWORD or not self . settings . USER : log . warning ( \"MILVUS_HOST, MILVUS_USER or MILVUS_PASSWORD for Milvus not provided. Thus running in non-credential Mode\" ) self . client : MilvusClient = MilvusClient ( uri = uri , user = self . settings . USER , password = self . settings . PASSWORD , timeout = self . milvus_timeout , ) self . collection_index : IndexParams = IndexParams ( ** self . settings . INDEX_PARAMS ) self . collection_history_len = self . settings . COLLECTION_HISTORY_LEN self . collection_prefix = self . settings . COLLECTION def __del__ ( self ): if getattr ( self , \"client\" , None ): self . client . close () def run ( self , inpt : DataFrame [ EmbeddingResult ]) -> MilvusResult : self . _insert_embeddings ( inpt ) try : old = self . __construct_last_collection_name () except NoPreviousCollection : old = \"\" self . _retire_collection () return MilvusResult ( new = self . __construct_current_collection_name (), old = old ) def _insert_embeddings ( self , data : pd . DataFrame ): collection_name = self . __construct_next_collection_name () log . info ( f \"Creating milvus collection { collection_name } \" ) collection_schema = CollectionSchema ( fields = [ FieldSchema ( name = \"pk\" , dtype = DataType . INT64 , is_primary = True , auto_id = True ), FieldSchema ( name = \"text\" , dtype = DataType . VARCHAR , max_length = 3000 ), FieldSchema ( name = \"vector\" , dtype = DataType . FLOAT_VECTOR , dim = len ( data [ \"vector\" ] . loc [ 0 ]), ), FieldSchema ( name = \"url\" , dtype = DataType . VARCHAR , max_length = 300 ), ], description = \"Collection for storing Milvus embeddings\" , ) log . info ( \"schema created\" ) self . client . create_collection ( collection_name = collection_name , schema = collection_schema ) log . info ( \"collection created\" ) log . info ( f \"Inserting embedding { len ( data ) } into collection { collection_name } \" ) result : dict = self . client . insert ( collection_name = collection_name , data = data . to_dict ( \"records\" )) if result [ \"insert_count\" ] != len ( data ): raise StepFailed ( f \"Failed to insert df into collection ' { collection_name } '. { result [ 'insert_count' ] } / { len ( data ) } where successful\" ) log . info ( f \"Successfully inserted { len ( data ) } vectors into collection ' { collection_name } '\" ) self . client . create_index ( collection_name = collection_name , index_params = self . collection_index ) log . info ( f \"Successfully craeted index { self . collection_index } into collection ' { collection_name } \" ) self . client . load_collection ( collection_name ) log . info ( f \"Successfully loaded the collection { collection_name } ' into collection ' { collection_name } '\" ) try : self . client . release_collection ( self . __construct_last_collection_name ()) except NoPreviousCollection : pass self . _update_alias ( collection_name ) def _retire_collection ( self ): collections_versioned : dict [ int , str ] = self . _get_collection_versions () to_delete = sorted ( collections_versioned . keys ())[: - self . collection_history_len ] if not to_delete : return for col_v in to_delete : col = collections_versioned [ col_v ] log . info ( f \"deleting { col } collection caused by retirement\" ) self . client . drop_collection ( col , timeout = self . milvus_timeout ) def _update_alias ( self , collection_name ): try : self . client . create_alias ( collection_name = collection_name , alias = self . collection_prefix , timeout = self . milvus_timeout , ) except MilvusException : self . client . alter_alias ( collection_name = collection_name , alias = self . collection_prefix , timeout = self . milvus_timeout , ) def __construct_next_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections : return f \" { self . collection_prefix } _v1\" previous_version = max ( previous_collections . keys ()) log . info ( f \"Found version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version + 1 } \" def __construct_last_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections or len ( previous_collections ) <= 1 : raise NoPreviousCollection ( f \"Milvus does not contain a previous collection for { self . collection_prefix } \" ) previous_version = sorted ( previous_collections . keys ())[ - 2 ] log . info ( f \"Found previous version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version } \" def __construct_current_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections or len ( previous_collections ) < 1 : raise NoPreviousCollection ( f \"Milvus does not contain a previous collection for { self . collection_prefix } \" ) previous_version = sorted ( previous_collections . keys ())[ - 1 ] log . info ( f \"Found previous version v { previous_version } \" ) return f \" { self . collection_prefix } _v { previous_version } \" def _get_collection_versions ( self ) -> dict [ int , str ]: previous_collections = self . client . list_collections ( timeout = self . milvus_timeout ) versioned_collections = { int ( previous . split ( \"_v\" )[ - 1 ]): previous for previous in previous_collections if self . collection_prefix in previous } return versioned_collections","title":"MilvusConnectorStep"},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings","text":"Bases: Settings MilvusSettings is a configuration class for managing settings related to MilvusDB. Attributes: HOST ( str ) \u2013 The hostname or IP address of the Milvus server. Defaults to \"localhost\". PORT ( int ) \u2013 The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530. COLLECTION ( str ) \u2013 The name of the collection in MilvusDB. COLLECTION_HISTORY_LEN ( int ) \u2013 The length of the collection history. Defaults to 10. SEARCH_PARAMS ( dict ) \u2013 Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS ( dict ) \u2013 Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\", \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}. USER ( str ) \u2013 The username for authentication with MilvusDB. PASSWORD ( str ) \u2013 The password for authentication with MilvusDB. SECURED ( bool ) \u2013 Indicates whether the connection to MilvusDB is secured. Defaults to False. Methods: Name Description parse_json Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Parameters: v ( str or dict ) \u2013 The value to validate and parse. Returns: dict \u2013 The parsed dictionary if the input is a JSON string, or the original value if it is already a dictionary. Source code in wurzel/steps/milvus/settings.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class MilvusSettings ( Settings ): \"\"\"MilvusSettings is a configuration class for managing settings related to MilvusDB. Attributes: HOST (str): The hostname or IP address of the Milvus server. Defaults to \"localhost\". PORT (int): The port number for the Milvus server. Must be between 1 and 65535. Defaults to 19530. COLLECTION (str): The name of the collection in MilvusDB. COLLECTION_HISTORY_LEN (int): The length of the collection history. Defaults to 10. SEARCH_PARAMS (dict): Parameters for search operations in MilvusDB. Defaults to {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS (dict): Parameters for indexing operations in MilvusDB. Defaults to {\"index_type\": \"FLAT\", \"field_name\": \"vector\", \"metric_type\": \"IP\", \"params\": {}}. USER (str): The username for authentication with MilvusDB. PASSWORD (str): The password for authentication with MilvusDB. SECURED (bool): Indicates whether the connection to MilvusDB is secured. Defaults to False. Methods: parse_json(cls, v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Args: v (str or dict): The value to validate and parse. Returns: dict: The parsed dictionary if the input is a JSON string, or the original value if it is already a dictionary. \"\"\" HOST : str = \"localhost\" PORT : int = Field ( 19530 , gt = 0 , le = 65535 ) COLLECTION : str COLLECTION_HISTORY_LEN : int = 10 SEARCH_PARAMS : dict = { \"metric_type\" : \"IP\" , \"params\" : {}} INDEX_PARAMS : dict = { \"index_type\" : \"FLAT\" , \"field_name\" : \"vector\" , \"metric_type\" : \"IP\" , \"params\" : {}, } USER : str PASSWORD : str SECURED : bool = False @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod # pylint: disable-next=R0801 def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"MilvusSettings"},{"location":"steps/milvus/#wurzel.steps.milvus.settings.MilvusSettings.parse_json","text":"Validation for json. Source code in wurzel/steps/milvus/settings.py 54 55 56 57 58 59 60 61 @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod # pylint: disable-next=R0801 def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"parse_json"},{"location":"steps/qdrant/","text":"containing the DVCStep sending embedding data into Qdrant. QdrantConnectorStep Bases: TypedStep [ QdrantSettings , DataFrame [ EmbeddingResult ], DataFrame [ QdrantResult ]] Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/qdrant/step.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 class QdrantConnectorStep ( TypedStep [ QdrantSettings , DataFrame [ EmbeddingResult ], DataFrame [ QdrantResult ]]): \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" _timeout : int = 20 s : QdrantSettings client : QdrantClient collection_name : str result_class = QdrantResult vector_key = \"vector\" def __init__ ( self ) -> None : super () . __init__ () # Qdrant stuff passed as environment # because we need to enject them into the DVC step during runtime, # not during DVC pipeline definition time # uri = \":memory:\" log . info ( f \"connecting to { self . settings . URI } \" ) if not self . settings . APIKEY : log . warning ( \"QDRANT__APIKEY for Qdrant not provided. Thus running in non-credential Mode\" ) self . client = QdrantClient ( location = self . settings . URI , api_key = self . settings . APIKEY , timeout = self . _timeout , ) self . collection_name = self . __construct_next_collection_name () self . id_iter = self . __id_gen () def __del__ ( self ): if getattr ( self , \"client\" , None ): self . client . close () def finalize ( self ) -> None : self . _create_indices () self . _update_alias () self . _retire_collections () return super () . finalize () def __id_gen ( self ): i = 0 while True : i += 1 yield i def run ( self , inpt : DataFrame [ EmbeddingResult ]) -> DataFrame [ QdrantResult ]: if not self . client . collection_exists ( self . collection_name ): self . _create_collection ( len ( inpt [ \"vector\" ] . loc [ 0 ])) df_result = self . _insert_embeddings ( inpt ) return df_result def _create_collection ( self , size : int ): log . debug ( f \"Creating Qdrant collection { self . collection_name } \" ) self . client . create_collection ( collection_name = self . collection_name , vectors_config = models . VectorParams ( size = size , distance = self . settings . DISTANCE ), replication_factor = self . settings . REPLICATION_FACTOR , ) def _get_entry_payload ( self , row : dict [ str , object ]) -> dict [ str , object ]: \"\"\"Create the payload for the entry.\"\"\" payload = { \"url\" : row [ \"url\" ], \"text\" : row [ \"text\" ], ** self . get_available_hashes ( row [ \"text\" ]), \"keywords\" : row [ \"keywords\" ], \"history\" : str ( step_history . get ()), } return payload def _create_point ( self , row : dict ) -> models . PointStruct : \"\"\"Creates a Qdrant PointStruct object from a given row dictionary. Args: row (dict): A dictionary representing a data entry, expected to contain at least the vector data under `self.vector_key`. Returns: models.PointStruct: An instance of PointStruct with a unique id, vector, and payload extracted from the row. Raises: KeyError: If the required vector key is not present in the row. \"\"\" payload = self . _get_entry_payload ( row ) return models . PointStruct ( id = next ( self . id_iter ), # type: ignore[arg-type] vector = row [ self . vector_key ], payload = payload , ) def _upsert_points ( self , points : list [ models . PointStruct ]): \"\"\"Inserts a list of points into the Qdrant collection in batches. Args: points (list[models.PointStruct]): The list of point structures to upsert into the collection. Raises: StepFailed: If any batch fails to be inserted into the collection. Logs: Logs a message for each successfully inserted batch, including the collection name and number of points. \"\"\" for point_chunk in _batch ( points , self . settings . BATCH_SIZE ): operation_info = self . client . upsert ( collection_name = self . collection_name , wait = True , points = point_chunk , ) if operation_info . status != \"completed\" : raise StepFailed ( f \"Failed to insert df chunk into collection ' { self . collection_name } ' { operation_info } \" ) log . info ( \"Successfully inserted vector_chunk\" , extra = { \"collection\" : self . collection_name , \"count\" : len ( point_chunk )}, ) def _build_result_dataframe ( self , points : list [ models . PointStruct ]): \"\"\"Constructs a DataFrame from a list of PointStruct objects. Each PointStruct's payload is unpacked into the resulting dictionary, along with its vector, collection name, and ID. The resulting list of dictionaries is used to create a DataFrame of the specified result_class. Args: points (list[models.PointStruct]): A list of PointStruct objects containing payload, vector, and id information. \"\"\" result_data = [ { ** entry . payload , self . vector_key : entry . vector , \"collection\" : self . collection_name , \"id\" : entry . id , } for entry in points ] return DataFrame [ self . result_class ]( result_data ) def _insert_embeddings ( self , data : DataFrame [ EmbeddingResult ]): log . info ( \"Inserting embeddings\" , extra = { \"count\" : len ( data ), \"collection\" : self . collection_name }) points = [ self . _create_point ( row ) for _ , row in data . iterrows ()] self . _upsert_points ( points ) return self . _build_result_dataframe ( points ) def _create_indices ( self ): self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"keywords\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . WHITESPACE , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"url\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . PREFIX , min_token_len = 3 , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"text\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . MULTILINGUAL , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"history\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . WORD ), ) def _retire_collections ( self ): collections_versioned : dict [ int , str ] = self . _get_collection_versions () to_delete = list ( collections_versioned . keys ())[: - self . settings . COLLECTION_HISTORY_LEN ] if not to_delete : return for col_v in to_delete : col = collections_versioned [ col_v ] log . info ( f \"deleting { col } collection caused by retirement\" ) self . client . delete_collection ( col ) def _update_alias ( self ): success = self . client . update_collection_aliases ( change_aliases_operations = [ models . CreateAliasOperation ( create_alias = models . CreateAlias ( collection_name = self . collection_name , alias_name = self . settings . COLLECTION , ) ) ] ) if not success : raise CustomQdrantException ( \"Alias Update failed\" ) def __construct_next_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections : return f \" { self . settings . COLLECTION } _v1\" previous_version = max ( previous_collections . keys ()) log . info ( f \"Found version v { previous_version } \" ) return f \" { self . settings . COLLECTION } _v { previous_version + 1 } \" def _get_collection_versions ( self ) -> dict [ int , str ]: previous_collections = self . client . get_collections () . collections versioned_collections = { int ( previous . name . split ( \"_v\" )[ - 1 ]): previous . name for previous in previous_collections if f \" { self . settings . COLLECTION } _v\" in previous . name } return dict ( sorted ( versioned_collections . items ())) @staticmethod def get_available_hashes ( text : str , encoding : str = \"utf-8\" ) -> dict : \"\"\"Compute `n` hashes for a given input text based. The number `n` depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported ## TLSH Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Args: text (str): Input text encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict[str, str]: keys: `text_<algo>_hash` hash as string ! Dict might be empty! \"\"\" hashes = {} encoded_text = text . encode ( encoding ) if HAS_TLSH : # pylint: disable=no-name-in-module, import-outside-toplevel from tlsh import hash as tlsh_hash hashes [ \"text_tlsh_hash\" ] = tlsh_hash ( encoded_text ) hashes [ \"text_sha256_hash\" ] = sha256 ( encoded_text ) . hexdigest () return hashes get_available_hashes ( text , encoding = 'utf-8' ) staticmethod Compute n hashes for a given input text based. The number n depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported TLSH Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Parameters: text ( str ) \u2013 Input text encoding ( str , default: 'utf-8' ) \u2013 Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict \u2013 dict[str, str]: keys: text_<algo>_hash hash as string ! Dict might be empty! Source code in wurzel/steps/qdrant/step.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 @staticmethod def get_available_hashes ( text : str , encoding : str = \"utf-8\" ) -> dict : \"\"\"Compute `n` hashes for a given input text based. The number `n` depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported ## TLSH Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Args: text (str): Input text encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict[str, str]: keys: `text_<algo>_hash` hash as string ! Dict might be empty! \"\"\" hashes = {} encoded_text = text . encode ( encoding ) if HAS_TLSH : # pylint: disable=no-name-in-module, import-outside-toplevel from tlsh import hash as tlsh_hash hashes [ \"text_tlsh_hash\" ] = tlsh_hash ( encoded_text ) hashes [ \"text_sha256_hash\" ] = sha256 ( encoded_text ) . hexdigest () return hashes containing the DVCStep sending embedding data into Qdrant. QdrantConnectorMultiVectorStep Bases: QdrantConnectorStep , TypedStep [ QdrantSettings , DataFrame [ EmbeddingMultiVectorResult ], DataFrame [ QdrantMultiVectorResult ]] Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/qdrant/step_multi_vector.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class QdrantConnectorMultiVectorStep ( QdrantConnectorStep , TypedStep [ QdrantSettings , DataFrame [ EmbeddingMultiVectorResult ], DataFrame [ QdrantMultiVectorResult ], ], ): \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" vector_key = \"vectors\" result_class = QdrantMultiVectorResult def _create_collection ( self , size : int ): self . client . create_collection ( collection_name = self . collection_name , vectors_config = models . VectorParams ( size = size , distance = self . settings . DISTANCE , multivector_config = models . MultiVectorConfig ( comparator = models . MultiVectorComparator . MAX_SIM ), ), replication_factor = self . settings . REPLICATION_FACTOR , ) def run ( self , inpt : DataFrame [ EmbeddingMultiVectorResult ]) -> DataFrame [ QdrantMultiVectorResult ]: log . debug ( f \"Creating Qdrant collection { self . collection_name } \" ) if not self . client . collection_exists ( self . collection_name ): self . _create_collection ( len ( inpt [ \"vectors\" ] . loc [ 0 ][ 0 ])) df_result = self . _insert_embeddings ( inpt ) return df_result def _get_entry_payload ( self , row : dict [ str , object ]) -> dict [ str , object ]: \"\"\"Create the payload for the entry.\"\"\" payload = super () . _get_entry_payload ( row ) payload [ \"splits\" ] = row [ \"splits\" ] return payload QdrantSettings Bases: Settings QdrantSettings is a configuration class for managing settings related to the Qdrant database. Attributes: DISTANCE ( Distance ) \u2013 The distance metric to be used, default is Distance.DOT. URI ( str ) \u2013 The URI for the Qdrant database, default is \"http://localhost:6333\". COLLECTION ( str ) \u2013 The name of the collection in the Qdrant database. COLLECTION_HISTORY_LEN ( int ) \u2013 The length of the collection history, default is 10. SEARCH_PARAMS ( dict ) \u2013 Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS ( dict ) \u2013 Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\". APIKEY ( Optional [ str ] ) \u2013 The API key for authentication, default is an empty string. REPLICATION_FACTOR ( int ) \u2013 The replication factor for the database, default is 3, must be greater than 0. BATCH_SIZE ( int ) \u2013 The batch size for operations, default is 1024, must be greater than 0. Methods: Name Description parse_json Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Parameters: v ( Union [ str , dict ] ) \u2013 The input value, either a JSON string or a dictionary. Returns: dict \u2013 The parsed dictionary if the input is a JSON string, otherwise the input value. Source code in wurzel/steps/qdrant/settings.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class QdrantSettings ( Settings ): \"\"\"QdrantSettings is a configuration class for managing settings related to the Qdrant database. Attributes: DISTANCE (Distance): The distance metric to be used, default is Distance.DOT. URI (str): The URI for the Qdrant database, default is \"http://localhost:6333\". COLLECTION (str): The name of the collection in the Qdrant database. COLLECTION_HISTORY_LEN (int): The length of the collection history, default is 10. SEARCH_PARAMS (dict): Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS (dict): Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\". APIKEY (Optional[str]): The API key for authentication, default is an empty string. REPLICATION_FACTOR (int): The replication factor for the database, default is 3, must be greater than 0. BATCH_SIZE (int): The batch size for operations, default is 1024, must be greater than 0. Methods: parse_json(v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Args: v (Union[str, dict]): The input value, either a JSON string or a dictionary. Returns: dict: The parsed dictionary if the input is a JSON string, otherwise the input value. \"\"\" DISTANCE : Distance = Distance . DOT URI : str = \"http://localhost:6333\" COLLECTION : str COLLECTION_HISTORY_LEN : int = 10 SEARCH_PARAMS : dict = { \"metric_type\" : \"IP\" , \"params\" : {}} INDEX_PARAMS : dict = { \"index_type\" : \"FLAT\" , \"field_name\" : \"vector\" , \"distance\" : \"Dot\" , \"params\" : {}, } APIKEY : Optional [ str ] = \"\" REPLICATION_FACTOR : int = Field ( default = 3 , gt = 0 ) BATCH_SIZE : int = Field ( default = 1024 , gt = 0 ) @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v parse_json ( v ) classmethod Validation for json. Source code in wurzel/steps/qdrant/settings.py 56 57 58 59 60 61 62 @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"Qdrant"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep","text":"Bases: TypedStep [ QdrantSettings , DataFrame [ EmbeddingResult ], DataFrame [ QdrantResult ]] Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/qdrant/step.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 class QdrantConnectorStep ( TypedStep [ QdrantSettings , DataFrame [ EmbeddingResult ], DataFrame [ QdrantResult ]]): \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" _timeout : int = 20 s : QdrantSettings client : QdrantClient collection_name : str result_class = QdrantResult vector_key = \"vector\" def __init__ ( self ) -> None : super () . __init__ () # Qdrant stuff passed as environment # because we need to enject them into the DVC step during runtime, # not during DVC pipeline definition time # uri = \":memory:\" log . info ( f \"connecting to { self . settings . URI } \" ) if not self . settings . APIKEY : log . warning ( \"QDRANT__APIKEY for Qdrant not provided. Thus running in non-credential Mode\" ) self . client = QdrantClient ( location = self . settings . URI , api_key = self . settings . APIKEY , timeout = self . _timeout , ) self . collection_name = self . __construct_next_collection_name () self . id_iter = self . __id_gen () def __del__ ( self ): if getattr ( self , \"client\" , None ): self . client . close () def finalize ( self ) -> None : self . _create_indices () self . _update_alias () self . _retire_collections () return super () . finalize () def __id_gen ( self ): i = 0 while True : i += 1 yield i def run ( self , inpt : DataFrame [ EmbeddingResult ]) -> DataFrame [ QdrantResult ]: if not self . client . collection_exists ( self . collection_name ): self . _create_collection ( len ( inpt [ \"vector\" ] . loc [ 0 ])) df_result = self . _insert_embeddings ( inpt ) return df_result def _create_collection ( self , size : int ): log . debug ( f \"Creating Qdrant collection { self . collection_name } \" ) self . client . create_collection ( collection_name = self . collection_name , vectors_config = models . VectorParams ( size = size , distance = self . settings . DISTANCE ), replication_factor = self . settings . REPLICATION_FACTOR , ) def _get_entry_payload ( self , row : dict [ str , object ]) -> dict [ str , object ]: \"\"\"Create the payload for the entry.\"\"\" payload = { \"url\" : row [ \"url\" ], \"text\" : row [ \"text\" ], ** self . get_available_hashes ( row [ \"text\" ]), \"keywords\" : row [ \"keywords\" ], \"history\" : str ( step_history . get ()), } return payload def _create_point ( self , row : dict ) -> models . PointStruct : \"\"\"Creates a Qdrant PointStruct object from a given row dictionary. Args: row (dict): A dictionary representing a data entry, expected to contain at least the vector data under `self.vector_key`. Returns: models.PointStruct: An instance of PointStruct with a unique id, vector, and payload extracted from the row. Raises: KeyError: If the required vector key is not present in the row. \"\"\" payload = self . _get_entry_payload ( row ) return models . PointStruct ( id = next ( self . id_iter ), # type: ignore[arg-type] vector = row [ self . vector_key ], payload = payload , ) def _upsert_points ( self , points : list [ models . PointStruct ]): \"\"\"Inserts a list of points into the Qdrant collection in batches. Args: points (list[models.PointStruct]): The list of point structures to upsert into the collection. Raises: StepFailed: If any batch fails to be inserted into the collection. Logs: Logs a message for each successfully inserted batch, including the collection name and number of points. \"\"\" for point_chunk in _batch ( points , self . settings . BATCH_SIZE ): operation_info = self . client . upsert ( collection_name = self . collection_name , wait = True , points = point_chunk , ) if operation_info . status != \"completed\" : raise StepFailed ( f \"Failed to insert df chunk into collection ' { self . collection_name } ' { operation_info } \" ) log . info ( \"Successfully inserted vector_chunk\" , extra = { \"collection\" : self . collection_name , \"count\" : len ( point_chunk )}, ) def _build_result_dataframe ( self , points : list [ models . PointStruct ]): \"\"\"Constructs a DataFrame from a list of PointStruct objects. Each PointStruct's payload is unpacked into the resulting dictionary, along with its vector, collection name, and ID. The resulting list of dictionaries is used to create a DataFrame of the specified result_class. Args: points (list[models.PointStruct]): A list of PointStruct objects containing payload, vector, and id information. \"\"\" result_data = [ { ** entry . payload , self . vector_key : entry . vector , \"collection\" : self . collection_name , \"id\" : entry . id , } for entry in points ] return DataFrame [ self . result_class ]( result_data ) def _insert_embeddings ( self , data : DataFrame [ EmbeddingResult ]): log . info ( \"Inserting embeddings\" , extra = { \"count\" : len ( data ), \"collection\" : self . collection_name }) points = [ self . _create_point ( row ) for _ , row in data . iterrows ()] self . _upsert_points ( points ) return self . _build_result_dataframe ( points ) def _create_indices ( self ): self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"keywords\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . WHITESPACE , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"url\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . PREFIX , min_token_len = 3 , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"text\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . MULTILINGUAL , ), ) self . client . create_payload_index ( collection_name = self . collection_name , field_name = \"history\" , field_schema = models . TextIndexParams ( type = models . TextIndexType . TEXT , tokenizer = models . TokenizerType . WORD ), ) def _retire_collections ( self ): collections_versioned : dict [ int , str ] = self . _get_collection_versions () to_delete = list ( collections_versioned . keys ())[: - self . settings . COLLECTION_HISTORY_LEN ] if not to_delete : return for col_v in to_delete : col = collections_versioned [ col_v ] log . info ( f \"deleting { col } collection caused by retirement\" ) self . client . delete_collection ( col ) def _update_alias ( self ): success = self . client . update_collection_aliases ( change_aliases_operations = [ models . CreateAliasOperation ( create_alias = models . CreateAlias ( collection_name = self . collection_name , alias_name = self . settings . COLLECTION , ) ) ] ) if not success : raise CustomQdrantException ( \"Alias Update failed\" ) def __construct_next_collection_name ( self ) -> str : previous_collections = self . _get_collection_versions () if not previous_collections : return f \" { self . settings . COLLECTION } _v1\" previous_version = max ( previous_collections . keys ()) log . info ( f \"Found version v { previous_version } \" ) return f \" { self . settings . COLLECTION } _v { previous_version + 1 } \" def _get_collection_versions ( self ) -> dict [ int , str ]: previous_collections = self . client . get_collections () . collections versioned_collections = { int ( previous . name . split ( \"_v\" )[ - 1 ]): previous . name for previous in previous_collections if f \" { self . settings . COLLECTION } _v\" in previous . name } return dict ( sorted ( versioned_collections . items ())) @staticmethod def get_available_hashes ( text : str , encoding : str = \"utf-8\" ) -> dict : \"\"\"Compute `n` hashes for a given input text based. The number `n` depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported ## TLSH Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Args: text (str): Input text encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict[str, str]: keys: `text_<algo>_hash` hash as string ! Dict might be empty! \"\"\" hashes = {} encoded_text = text . encode ( encoding ) if HAS_TLSH : # pylint: disable=no-name-in-module, import-outside-toplevel from tlsh import hash as tlsh_hash hashes [ \"text_tlsh_hash\" ] = tlsh_hash ( encoded_text ) hashes [ \"text_sha256_hash\" ] = sha256 ( encoded_text ) . hexdigest () return hashes","title":"QdrantConnectorStep"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes","text":"Compute n hashes for a given input text based. The number n depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported","title":"get_available_hashes"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step.QdrantConnectorStep.get_available_hashes--tlsh","text":"Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Parameters: text ( str ) \u2013 Input text encoding ( str , default: 'utf-8' ) \u2013 Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict \u2013 dict[str, str]: keys: text_<algo>_hash hash as string ! Dict might be empty! Source code in wurzel/steps/qdrant/step.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 @staticmethod def get_available_hashes ( text : str , encoding : str = \"utf-8\" ) -> dict : \"\"\"Compute `n` hashes for a given input text based. The number `n` depends on the optionally installed python libs. For now only TLSH (Trend Micro Locality Sensitive Hash) is supported ## TLSH Given a byte stream with a minimum length of 50 bytes TLSH generates a hash value which can be used for similarity comparisons. Args: text (str): Input text encoding (str, optional): Input text will encoded to bytes using this encoding. Defaults to \"utf-8\". Returns: dict[str, str]: keys: `text_<algo>_hash` hash as string ! Dict might be empty! \"\"\" hashes = {} encoded_text = text . encode ( encoding ) if HAS_TLSH : # pylint: disable=no-name-in-module, import-outside-toplevel from tlsh import hash as tlsh_hash hashes [ \"text_tlsh_hash\" ] = tlsh_hash ( encoded_text ) hashes [ \"text_sha256_hash\" ] = sha256 ( encoded_text ) . hexdigest () return hashes containing the DVCStep sending embedding data into Qdrant.","title":"TLSH"},{"location":"steps/qdrant/#wurzel.steps.qdrant.step_multi_vector.QdrantConnectorMultiVectorStep","text":"Bases: QdrantConnectorStep , TypedStep [ QdrantSettings , DataFrame [ EmbeddingMultiVectorResult ], DataFrame [ QdrantMultiVectorResult ]] Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings. Source code in wurzel/steps/qdrant/step_multi_vector.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class QdrantConnectorMultiVectorStep ( QdrantConnectorStep , TypedStep [ QdrantSettings , DataFrame [ EmbeddingMultiVectorResult ], DataFrame [ QdrantMultiVectorResult ], ], ): \"\"\"Qdrant connector step. It consumes embedding csv files, creates a new schema and inserts the embeddings.\"\"\" vector_key = \"vectors\" result_class = QdrantMultiVectorResult def _create_collection ( self , size : int ): self . client . create_collection ( collection_name = self . collection_name , vectors_config = models . VectorParams ( size = size , distance = self . settings . DISTANCE , multivector_config = models . MultiVectorConfig ( comparator = models . MultiVectorComparator . MAX_SIM ), ), replication_factor = self . settings . REPLICATION_FACTOR , ) def run ( self , inpt : DataFrame [ EmbeddingMultiVectorResult ]) -> DataFrame [ QdrantMultiVectorResult ]: log . debug ( f \"Creating Qdrant collection { self . collection_name } \" ) if not self . client . collection_exists ( self . collection_name ): self . _create_collection ( len ( inpt [ \"vectors\" ] . loc [ 0 ][ 0 ])) df_result = self . _insert_embeddings ( inpt ) return df_result def _get_entry_payload ( self , row : dict [ str , object ]) -> dict [ str , object ]: \"\"\"Create the payload for the entry.\"\"\" payload = super () . _get_entry_payload ( row ) payload [ \"splits\" ] = row [ \"splits\" ] return payload","title":"QdrantConnectorMultiVectorStep"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings","text":"Bases: Settings QdrantSettings is a configuration class for managing settings related to the Qdrant database. Attributes: DISTANCE ( Distance ) \u2013 The distance metric to be used, default is Distance.DOT. URI ( str ) \u2013 The URI for the Qdrant database, default is \"http://localhost:6333\". COLLECTION ( str ) \u2013 The name of the collection in the Qdrant database. COLLECTION_HISTORY_LEN ( int ) \u2013 The length of the collection history, default is 10. SEARCH_PARAMS ( dict ) \u2013 Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS ( dict ) \u2013 Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\". APIKEY ( Optional [ str ] ) \u2013 The API key for authentication, default is an empty string. REPLICATION_FACTOR ( int ) \u2013 The replication factor for the database, default is 3, must be greater than 0. BATCH_SIZE ( int ) \u2013 The batch size for operations, default is 1024, must be greater than 0. Methods: Name Description parse_json Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Parameters: v ( Union [ str , dict ] ) \u2013 The input value, either a JSON string or a dictionary. Returns: dict \u2013 The parsed dictionary if the input is a JSON string, otherwise the input value. Source code in wurzel/steps/qdrant/settings.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class QdrantSettings ( Settings ): \"\"\"QdrantSettings is a configuration class for managing settings related to the Qdrant database. Attributes: DISTANCE (Distance): The distance metric to be used, default is Distance.DOT. URI (str): The URI for the Qdrant database, default is \"http://localhost:6333\". COLLECTION (str): The name of the collection in the Qdrant database. COLLECTION_HISTORY_LEN (int): The length of the collection history, default is 10. SEARCH_PARAMS (dict): Parameters for search operations, default is {\"metric_type\": \"IP\", \"params\": {}}. INDEX_PARAMS (dict): Parameters for index creation, default includes \"index_type\", \"field_name\", \"distance\", and \"params\". APIKEY (Optional[str]): The API key for authentication, default is an empty string. REPLICATION_FACTOR (int): The replication factor for the database, default is 3, must be greater than 0. BATCH_SIZE (int): The batch size for operations, default is 1024, must be greater than 0. Methods: parse_json(v): Validates and parses JSON strings into Python objects for SEARCH_PARAMS and INDEX_PARAMS. Args: v (Union[str, dict]): The input value, either a JSON string or a dictionary. Returns: dict: The parsed dictionary if the input is a JSON string, otherwise the input value. \"\"\" DISTANCE : Distance = Distance . DOT URI : str = \"http://localhost:6333\" COLLECTION : str COLLECTION_HISTORY_LEN : int = 10 SEARCH_PARAMS : dict = { \"metric_type\" : \"IP\" , \"params\" : {}} INDEX_PARAMS : dict = { \"index_type\" : \"FLAT\" , \"field_name\" : \"vector\" , \"distance\" : \"Dot\" , \"params\" : {}, } APIKEY : Optional [ str ] = \"\" REPLICATION_FACTOR : int = Field ( default = 3 , gt = 0 ) BATCH_SIZE : int = Field ( default = 1024 , gt = 0 ) @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"QdrantSettings"},{"location":"steps/qdrant/#wurzel.steps.qdrant.settings.QdrantSettings.parse_json","text":"Validation for json. Source code in wurzel/steps/qdrant/settings.py 56 57 58 59 60 61 62 @field_validator ( \"SEARCH_PARAMS\" , \"INDEX_PARAMS\" , mode = \"before\" ) @classmethod def parse_json ( cls , v ): \"\"\"Validation for json.\"\"\" if isinstance ( v , str ): return json . loads ( v ) return v","title":"parse_json"}]}